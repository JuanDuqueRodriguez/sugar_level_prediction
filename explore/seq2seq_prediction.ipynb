{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"../data/\"\n",
    "noise=0.0\n",
    "history = 100 \n",
    "future = 12\n",
    "train_batch = 200\n",
    "test_batch = 200\n",
    "num_epochs = 5\n",
    "\n",
    "train = np.load(os.path.join(root, \"noisy%s_train.npy\" %noise))\n",
    "vad = np.load(os.path.join(root, \"noisy%s_vad.npy\" %noise))\n",
    "test = np.load(os.path.join(root, \"noisy%s_test.npy\" %noise))\n",
    "\n",
    "train_steps = train.shape[0] // train_batch\n",
    "vad_steps = vad.shape[0] // train_batch\n",
    "test_steps = test.shape[0] // test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70350, 224), (9365, 224), (8781, 224))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, vad.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(data):\n",
    "    return tf.data.Dataset.from_tensor_slices(data)\n",
    "\n",
    "def make_iterator(dataset, batch_size, num_epochs):\n",
    "    return dataset.batch(batch_size).repeat(num_epochs).make_one_shot_iterator().get_next()\n",
    "\n",
    "def split(data, history, future, batch_size, num_epochs=1):\n",
    "    xf, xl = data[:, :history], data[:, history:history+future]\n",
    "    yf, yl = data[:, history+future:2*history+future], data[:, 2*history+future:]\n",
    "    # add start of sentence to labels (input to the decoder)\n",
    "    yl_input = np.zeros(shape=(yl.shape[0], yl.shape[1]+1))\n",
    "    yl_input[:, 1:] = yl\n",
    "    # add end of sentence to labels (output of the decoder)\n",
    "    yl_output = np.zeros(shape=(yl.shape[0], yl.shape[1]+1))\n",
    "    yl_output[:, :-1] = yl\n",
    "    \n",
    "    # add new dimension at the end of all arrays\n",
    "    yf = yf[:, :, np.newaxis]\n",
    "    yl_input = yl_input[:, :, np.newaxis].astype(np.float32)\n",
    "    yl_output = yl_output[:, :, np.newaxis].astype(np.float32)\n",
    "    \n",
    "    return (yf, yl_input), yl_output\n",
    "\n",
    "train_gen = split(train, history, future, train_batch, num_epochs)\n",
    "vad_gen = split(vad, history, future, train_batch, 1)\n",
    "test_gen = split(test, history, future, test_batch, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70350, 100, 1), (70350, 13, 1), (70350, 13, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gen[0][0].shape, train_gen[0][1].shape, train_gen[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Dense, LSTM, Input\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.losses import MAE\n",
    "from tensorflow.python.keras.metrics import MAPE\n",
    "from tensorflow.python.keras.backend import clear_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq(history, future, latent_dim):\n",
    "    clear_session()\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(history, 1))\n",
    "    encoder = LSTM(latent_dim, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder, using `encoder_states` as initial state.\n",
    "    decoder_inputs = Input(shape=(future+1, 1))\n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the \n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_dense = Dense(1, activation='selu')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70350 samples, validate on 9365 samples\n",
      "Epoch 1/5\n",
      "70350/70350 [==============================] - 228s 3ms/step - loss: 0.1168 - mean_absolute_percentage_error: 28422752.0144 - mean_absolute_error: 0.1168 - val_loss: 0.0772 - val_mean_absolute_percentage_error: 18801599.7768 - val_mean_absolute_error: 0.0772\n",
      "Epoch 2/5\n",
      "70350/70350 [==============================] - 224s 3ms/step - loss: 0.0666 - mean_absolute_percentage_error: 17078473.5124 - mean_absolute_error: 0.0666 - val_loss: 0.0664 - val_mean_absolute_percentage_error: 10031653.0635 - val_mean_absolute_error: 0.0664\n",
      "Epoch 3/5\n",
      "70350/70350 [==============================] - 197s 3ms/step - loss: 0.0553 - mean_absolute_percentage_error: 13029406.1173 - mean_absolute_error: 0.0553 - val_loss: 0.0525 - val_mean_absolute_percentage_error: 14263808.8206 - val_mean_absolute_error: 0.0525\n",
      "Epoch 4/5\n",
      "70350/70350 [==============================] - 204s 3ms/step - loss: 0.0477 - mean_absolute_percentage_error: 10461799.7569 - mean_absolute_error: 0.0477 - val_loss: 0.0479 - val_mean_absolute_percentage_error: 15906579.9658 - val_mean_absolute_error: 0.0479\n",
      "Epoch 5/5\n",
      "70350/70350 [==============================] - 208s 3ms/step - loss: 0.0424 - mean_absolute_percentage_error: 7575081.4666 - mean_absolute_error: 0.0424 - val_loss: 0.0418 - val_mean_absolute_percentage_error: 4585248.3903 - val_mean_absolute_error: 0.0418\n"
     ]
    }
   ],
   "source": [
    "m2 = seq2seq(history, future, latent_dim=50)\n",
    "m2.compile(optimizer='rmsprop', loss=MAE, metrics=[MAPE, MAE])\n",
    "h2 = m2.fit(x=train_gen[0], y=train_gen[1], batch_size=train_batch, epochs=num_epochs, \n",
    "      validation_data=vad_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70350 samples, validate on 9365 samples\n",
      "Epoch 1/5\n",
      "70350/70350 [==============================] - 213s 3ms/step - loss: 734721.6606 - mean_absolute_percentage_error: 734721.6606 - mean_absolute_error: 0.4195 - val_loss: 169410.8792 - val_mean_absolute_percentage_error: 169410.8792 - val_mean_absolute_error: 0.4177\n",
      "Epoch 2/5\n",
      "70350/70350 [==============================] - 201s 3ms/step - loss: 401224.5604 - mean_absolute_percentage_error: 401224.5604 - mean_absolute_error: 0.4178 - val_loss: 617839.9356 - val_mean_absolute_percentage_error: 617839.9356 - val_mean_absolute_error: 0.4112\n",
      "Epoch 3/5\n",
      "70350/70350 [==============================] - 202s 3ms/step - loss: 309001.9542 - mean_absolute_percentage_error: 309001.9542 - mean_absolute_error: 0.4177 - val_loss: 105411.4461 - val_mean_absolute_percentage_error: 105411.4461 - val_mean_absolute_error: 0.4193\n",
      "Epoch 4/5\n",
      "70350/70350 [==============================] - 203s 3ms/step - loss: 249273.3744 - mean_absolute_percentage_error: 249273.3744 - mean_absolute_error: 0.4175 - val_loss: 327002.3357 - val_mean_absolute_percentage_error: 327002.3357 - val_mean_absolute_error: 0.4135\n",
      "Epoch 5/5\n",
      "70350/70350 [==============================] - 222s 3ms/step - loss: 214279.1935 - mean_absolute_percentage_error: 214279.1935 - mean_absolute_error: 0.4175 - val_loss: 245959.5692 - val_mean_absolute_percentage_error: 245959.5692 - val_mean_absolute_error: 0.4153\n"
     ]
    }
   ],
   "source": [
    "m3 = seq2seq(history, future, latent_dim=50)\n",
    "m3.compile(optimizer='rmsprop', loss=MAPE, metrics=[MAPE, MAE])\n",
    "h3 = m3.fit(x=train_gen[0], y=train_gen[1], batch_size=train_batch, epochs=num_epochs, \n",
    "      validation_data=vad_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70350 samples, validate on 9365 samples\n",
      "Epoch 1/5\n",
      "70350/70350 [==============================] - 407s 6ms/step - loss: 0.1085 - mean_absolute_percentage_error: 26867136.7138 - mean_absolute_error: 0.1085 - val_loss: 0.0746 - val_mean_absolute_percentage_error: 20523189.9221 - val_mean_absolute_error: 0.0746\n",
      "Epoch 2/5\n",
      "70350/70350 [==============================] - 401s 6ms/step - loss: 0.0642 - mean_absolute_percentage_error: 19557340.7932 - mean_absolute_error: 0.0642 - val_loss: 0.0657 - val_mean_absolute_percentage_error: 33651767.3134 - val_mean_absolute_error: 0.0657\n",
      "Epoch 3/5\n",
      "70350/70350 [==============================] - 401s 6ms/step - loss: 0.0505 - mean_absolute_percentage_error: 13386543.2793 - mean_absolute_error: 0.0505 - val_loss: 0.0428 - val_mean_absolute_percentage_error: 6656499.4861 - val_mean_absolute_error: 0.0428\n",
      "Epoch 4/5\n",
      "70350/70350 [==============================] - 398s 6ms/step - loss: 0.0425 - mean_absolute_percentage_error: 7110788.7436 - mean_absolute_error: 0.0425 - val_loss: 0.0388 - val_mean_absolute_percentage_error: 3930740.2112 - val_mean_absolute_error: 0.0388\n",
      "Epoch 5/5\n",
      "70350/70350 [==============================] - 367s 5ms/step - loss: 0.0385 - mean_absolute_percentage_error: 5725695.5741 - mean_absolute_error: 0.0385 - val_loss: 0.0423 - val_mean_absolute_percentage_error: 8874910.2440 - val_mean_absolute_error: 0.0423\n"
     ]
    }
   ],
   "source": [
    "m4 = seq2seq(history, future, latent_dim=100)\n",
    "m4.compile(optimizer='rmsprop', loss=MAE, metrics=[MAPE, MAE])\n",
    "h4 = m4.fit(x=train_gen[0], y=train_gen[1], batch_size=train_batch, epochs=num_epochs, \n",
    "      validation_data=vad_gen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ks_dl_course]",
   "language": "python",
   "name": "conda-env-ks_dl_course-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
