{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "seq2seq_prediction_attention_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSmCiYhICymz",
        "colab_type": "text"
      },
      "source": [
        "# Sequence to Sequence models for Blood Glucose prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ykin6dDKdff",
        "colab_type": "text"
      },
      "source": [
        "## Intro\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAoGrFpKKbz1",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Seq2Seq modelling with Recurrent Neural Networks, allowing for attention mechanisim and teacher-forcing.\n",
        "\n",
        "We pose the problem of Blood Glucose (BG) estimation with X hours in advance as a Seq2Seq auto-regressive problem. The architecture consists of:\n",
        "\n",
        "* `ProcessInput` module: It creates embeddings for categorical variables, concatenates them with numerical ones, and pass the result through a Dense layer (this being otional). Here, we could also add a block of 1-dimensional Convolutional Neural Networks, to feed subsequent modules with an enriched set of features.\n",
        "\n",
        "* `Encoder` module: an stack of Long-short terrm memory (LSTM) recurrent networks, that retrieves output sequences (of length equal to the input sequences), as well as the last states.\n",
        "\n",
        "* `Attention` module: here we implement the three most used attention mechanisms, that is, additive, general and scaled dot-product. \n",
        "\n",
        "* `Decoder`module:\n",
        "\n",
        "\n",
        "In our experiemnts, we do not observe any improvements when using the attention mechanism, but rather an increment in the time required for training. However, we have not extensively covered the hyper-parameter space, and thus there is room for improvement.\n",
        "\n",
        "Regarding teacher forcing, since our task is to generate sequences without making use of the ground truth, we do observe a deterioration in the forecasting. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02lz22M8Kvr6",
        "colab_type": "text"
      },
      "source": [
        "### Results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_ahMPfwFF9O",
        "colab_type": "text"
      },
      "source": [
        "Predict BG 3 hours in advance:\n",
        "\n",
        "  * 1 day of historic data: MAE=32.96 mg/dl;, MAPE=25,94%\n",
        "  * 3 day of historic data:\n",
        "  * 6 day of historic data:\n",
        "\n",
        "Predict BG 1 hour in advance:\n",
        "\n",
        "  * 1 day of historic data:\n",
        "  * 3 day of historic data:\n",
        "  * 6 day of historic data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KJnrtvZE7XW",
        "colab_type": "text"
      },
      "source": [
        "### Bibliography\n",
        "\n",
        "\n",
        "Some papers and blogs that I have followed: \n",
        "\n",
        "\n",
        "* https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
        "\n",
        "* https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3\n",
        "\n",
        "* https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
        "\n",
        "* https://arxiv.org/pdf/1508.04025v5.pdf\n",
        "\n",
        "* https://arxiv.org/pdf/1506.03099.pdf\n",
        "\n",
        "* https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html\n",
        "\n",
        "Some interesting papers applying RNN to EHR data:\n",
        "\n",
        "* https://arxiv.org/pdf/1711.03905.pdf \n",
        "\n",
        "* https://arxiv.org/pdf/1902.10877.pdf\n",
        "\n",
        "* https://arxiv.org/abs/1511.03677\n",
        "\n",
        "And timeseries anlysis in financial data with RNNs/attention:\n",
        "\n",
        "* https://iopscience.iop.org/article/10.1088/1757-899X/569/5/052037/pdf\n",
        "\n",
        "* https://arxiv.org/pdf/1902.10877.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Bf6QtFIDWP_",
        "colab_type": "code",
        "outputId": "79072219-5001-4a31-aeb6-26bfc5647182",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNKe6wt8Cym-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import copy\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZTMhkmyDR0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Dense, LSTM, Input, RepeatVector, concatenate\n",
        "from tensorflow.keras.layers import Embedding, GaussianNoise, Reshape, Dropout \n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import MAE\n",
        "from tensorflow.keras.metrics import MAPE, MSE\n",
        "from tensorflow.keras.backend import clear_session\n",
        "from tensorflow.keras.constraints import max_norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2vCNO9-PfN3",
        "colab_type": "text"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3KZB7NjEIl0",
        "colab_type": "text"
      },
      "source": [
        "### Load datasets\n",
        "\n",
        "Each dataset consits of sequences of `history`+`future` points, with 5 features (i.e. matrices of shape=(num_sequences, `history`+`future`, 5)) : \n",
        "\n",
        "* time interval: days counted starting from the end of the `history` of the sequence. Thus, for points in the `history`, this feature takes negatuve values, while for points in the `future`, it's positive. \n",
        "* hour: hour of the day\n",
        "* day of week: day of the week in numbers ('Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3, 'Friday': 4, 'Saturday': 5, 'Sunday': 6)\n",
        "* patient_id: numeric identifier for the patient, starting at zero.\n",
        "* sugar level: recorded sugar level, scaled with min/max scaler.\n",
        "\n",
        "Additionally, we add two columns for the scale (min and max) of every sequence, so that the actual shape of the datasets is (num_sequences, `history`+`future`+2, 5). The scale is thus repeated 5 times (one for each feature)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRgpiZSoEH2x",
        "colab_type": "code",
        "outputId": "545f170e-5547-4f05-ea74-9a886ef21ae9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vdrtT6jCynO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root = \"/content/drive/My Drive/Colab Notebooks/sugar_level_prediction/data/processed\"\n",
        "lastdate = '2020-01-24'\n",
        "root = os.path.join(root, lastdate)\n",
        "\n",
        "days = 3\n",
        "hours = 3\n",
        "noise = 0.0\n",
        "replace = False\n",
        "name = \"{{dataset}}_noise_{noise}_feature-{days}days_label-{hours}hours_replace{replace}.npy\" \n",
        "name = name.format(noise=noise, days=days, hours=hours, replace=replace)\n",
        "\n",
        "history = days * 24 * 4\n",
        "future = hours * 4\n",
        "\n",
        "train = np.load(os.path.join(root, name.format(dataset='train')))\n",
        "vad = np.load(os.path.join(root, name.format(dataset='vad')))\n",
        "test = np.load(os.path.join(root, name.format(dataset='test')))\n",
        "\n",
        "assert history+future+2 == train.shape[1]\n",
        "assert 5 == train.shape[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_mOC2M-qeTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1203)\n",
        "np.random.shuffle(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGcDkv-LJWUw",
        "colab_type": "text"
      },
      "source": [
        "###  Split data \n",
        "\n",
        "Each input sequence has both the features and labels (x and y, if you wish), so we have to separate them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk_8mzP9JrcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_features_labels(data, history, future, start_char):\n",
        "    \"\"\"\n",
        "    Method to separate historic and future events (features and labels). \n",
        "    It returns input data for the encoder and decoder, and the output data \n",
        "    for the decoder. The input data for the decoder is just the output data \n",
        "    of the decoder, shifted by one step. \n",
        "\n",
        "    :param data: numpy ndarray with sequences of history+future points, and 4 attributes \n",
        "      (time_interval, hour_of_day, day_of_week, patient_id, sugar_level). It has shape (?, history+future, 5)\n",
        "    :param history: number of points for the features\n",
        "    :param future: number of points for the labels\n",
        "    :param start_char: start charcter for input sequences to the decoder\n",
        "    :return three numpy arrays with the input data for the encoder (shape=(?, history, 5))\n",
        "        and decoder (shape=(?, future+1, 1)), and the output data for the decoder\n",
        "        (shape=(?, future+1, 1))\n",
        "    \"\"\"\n",
        "    # split features and labels. Note that for the later, we only keep the \n",
        "    # feature with the sugar level, which constitutes our target\n",
        "    yfeature, ylabel = data[:, :history], data[:, history:history+future, -1]\n",
        "    \n",
        "    # get scaler\n",
        "    scaler = data[:, history+future:history+future+2, -1]\n",
        "    \n",
        "    # add new dimension at the end of the output arrays to the decoder\n",
        "    ylabel = ylabel[:, :, np.newaxis].astype(np.float32)\n",
        "    \n",
        "    # add 2 new dimensions at the end of the scaler\n",
        "    scaler = scaler[:, :, np.newaxis, np.newaxis]\n",
        "    return yfeature, ylabel, scaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE0-L8XQObFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_char = -1.0\n",
        "train_data = split_features_labels(train, history, future, start_char)\n",
        "vad_data = split_features_labels(vad, history, future, start_char)\n",
        "test_data = split_features_labels(test, history, future, start_char)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26Ib_uLdCyns",
        "colab_type": "code",
        "outputId": "7b6e618b-228c-4019-8d0a-e73b87ae1889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data[0].shape, train_data[1].shape, train_data[2].shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((343492, 288, 5), (343492, 12, 1), (343492, 2, 1, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5YjYEbSDohP",
        "colab_type": "text"
      },
      "source": [
        "### Generate datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7ktp8XoLq0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_columns(data):\n",
        "    \"\"\"\n",
        "    Split and prepare input data\n",
        "\n",
        "    :param data: array with four columns: time_interval, hour_of_day, \n",
        "        day_of_week, sugar_level\n",
        "    \"\"\"\n",
        "    time_interval, hour_of_day, day_of_week, patient_id, sugar_level = np.split(data, \n",
        "        indices_or_sections=5, axis=2)\n",
        "    # squeeze categorical features (ncessary for the Embedding layer)\n",
        "    hour_of_day = np.squeeze(hour_of_day, axis=2)\n",
        "    day_of_week = np.squeeze(day_of_week, axis=2)\n",
        "    patient_id = np.squeeze(patient_id, axis=2)\n",
        "    return (time_interval, hour_of_day, day_of_week, patient_id, sugar_level)\n",
        "\n",
        "def get_dataset(data, batch_size, shuffle=False, drop_remainder=False):\n",
        "  input_ = get_columns(data[0])\n",
        "  target_ = data[1]\n",
        "  scaler_ = data[2]\n",
        "  buffer_size = input_[0].shape[0]\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((input_, target_, scaler_))\n",
        "  if shuffle:\n",
        "    dataset = dataset.shuffle(buffer_size)\n",
        "  dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRCUDHegdANq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 200\n",
        "steps_per_epoch = train_data[0].shape[0] // batch_size\n",
        "train_dataset = get_dataset(train_data, batch_size, shuffle=True, drop_remainder=True) \n",
        "vad_dataset = get_dataset(vad_data, batch_size, shuffle=False, drop_remainder=False) \n",
        "test_dataset = get_dataset(test_data, batch_size, shuffle=False, drop_remainder=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZF3oU66i4M6",
        "colab_type": "code",
        "outputId": "741765d1-30f9-4950-a7c4-3a4f0a0591c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "num_identifiers = np.unique(train_data[0][:, :, 3]).size\n",
        "print(\"There are %s individuals\" % num_identifiers)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 10 individuals\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiizJISGiUyV",
        "colab_type": "text"
      },
      "source": [
        "## Some auxiliar functions \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3vlqUipiiLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_train_history(history, title, metric):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    loss = history[metric]\n",
        "    val_loss = history['val_%s' %metric]\n",
        "\n",
        "    epochs = range(len(loss))\n",
        "\n",
        "    plt.plot(epochs, loss, 'b', label='Training %s' %metric)\n",
        "    plt.plot(epochs, val_loss, 'r', label='Validation %s' %metric)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.yscale('log')\n",
        "    plt.gca().grid(axis='y', which='minor')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def rescale_func(y, scale):\n",
        "    return scale[0] + (scale[1] - scale[0])*y\n",
        "\n",
        "def plot_results(dataset, prediction, history, future):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # get random idx\n",
        "    idx = np.random.randint(0, dataset.shape[0])\n",
        "\n",
        "    # get intervals\n",
        "    xf = dataset[idx, :history, 0]\n",
        "    xl = dataset[idx, history:history+future, 0]\n",
        "    \n",
        "    # get sequences\n",
        "    input_seq_ = dataset[idx, :history, -1]\n",
        "    target_seq_ = dataset[idx, history:history+future, -1]\n",
        "    decoded_seq_ = np.squeeze(prediction[idx, :])\n",
        "    \n",
        "    # rescale: note that predictions are already rescaled\n",
        "    scale = dataset[idx, history+future:history+future+2, -1]\n",
        "    input_seq_ = rescale_func(input_seq_, scale)\n",
        "    target_seq_ = rescale_func(target_seq_, scale)\n",
        "\n",
        "\n",
        "    ymin = scale[0] - 10\n",
        "    ymax = scale[1] + 10\n",
        "    \n",
        "    plt.plot(xf, input_seq_, '-b', markersize=2, linewidth=2, label='historic data')\n",
        "    plt.plot(xl, target_seq_, '.--b', markersize=2, linewidth=2, label='future data')\n",
        "    plt.plot(xl, decoded_seq_, '.--k', markersize=2, linewidth=2, label='prediction')\n",
        "    plt.vlines(x=0, ymin=ymin, ymax=ymax, colors='k', linewidth=1, linestyles='--')\n",
        "\n",
        "    plt.ylim(ymin, ymax)\n",
        "    plt.xlim(-0.3-history/4/24, future/4/24+0.3)\n",
        "    \n",
        "    plt.ylabel('Glucose evolution (arb.units)')\n",
        "    plt.xlabel('Days before/after start prediction')\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlM_vedWxNDR",
        "colab_type": "text"
      },
      "source": [
        "## Seq2Seq model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jma2DOYxLVFg",
        "colab_type": "text"
      },
      "source": [
        "### Class ProcessInput "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rYj5zjFxlS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ProcessInput(tf.keras.Model):\n",
        "  def __init__(self, dense_dim, dense_activation, gaussian_noise, num_ids,\n",
        "               day_of_week_dim, hour_of_day_dim, patient_id_dim,\n",
        "               **kwargs):\n",
        "    super(ProcessInput, self).__init__(**kwargs)\n",
        "\n",
        "    # define attribute variables\n",
        "    self.dense_dim = dense_dim\n",
        "    self.dense_activation = dense_activation\n",
        "    self.gaussian_noise = gaussian_noise\n",
        "    self.num_ids = num_ids\n",
        "    self.day_of_week_dim = day_of_week_dim \n",
        "    self.hour_of_day_dim = hour_of_day_dim \n",
        "    self.patient_id_dim = patient_id_dim\n",
        "\n",
        "    # Define embedding layers for Input categorical layers\n",
        "    self.day_of_week_emb = Embedding(input_dim=7, output_dim=self.day_of_week_dim)\n",
        "    self.hour_of_day_emb = Embedding(input_dim=24, output_dim=self.hour_of_day_dim)\n",
        "    self.patient_id_emb = Embedding(input_dim=self.num_ids, output_dim=self.patient_id_dim)\n",
        "\n",
        "    # gaussian noise layer \n",
        "    if self.gaussian_noise: \n",
        "      self.gn_layer = GaussianNoise(stddev=self.gaussian_noise)\n",
        "    else: \n",
        "      self.gn_layer = None\n",
        "    # dense layer\n",
        "    if self.dense_dim: \n",
        "      # TODO use TimeDistributed before the Dense? I believe the layer is broadcasted here\n",
        "      self.dense = Dense(self.dense_dim, activation=self.dense_activation)\n",
        "    else:\n",
        "        self.dense = None\n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    # split inputs\n",
        "    time_interval, hour_of_day, day_of_week, patient_id, sugar_level = inputs\n",
        "    # create embeddings for categorical variables\n",
        "    day_of_week = self.day_of_week_emb(day_of_week)\n",
        "    hour_of_day = self.hour_of_day_emb(hour_of_day)\n",
        "    patient_id = self.patient_id_emb(patient_id)\n",
        "\n",
        "    # add noise\n",
        "    if self.gn_layer:\n",
        "        sugar_level = self.gn_layer(sugar_level, training=training)\n",
        "    \n",
        "    # Concatenate all inputs toghether, and pass them through a dense layer\n",
        "    output = concatenate([time_interval, hour_of_day, day_of_week, \n",
        "                          patient_id, sugar_level], axis=-1)\n",
        "    if self.dense:\n",
        "        output = self.dense(output)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RETckryiLdQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# simple testing\n",
        "test_layer = ProcessInput(dense_dim=None, dense_activation=None, gaussian_noise=0.1, \n",
        "                          num_ids=7, day_of_week_dim=10, hour_of_day_dim=10, \n",
        "                          patient_id_dim=10)\n",
        "\n",
        "assert test_layer.dense == None\n",
        "assert test_layer.gaussian_noise == 0.1\n",
        "assert test_layer.gn_layer.stddev == 0.1\n",
        "\n",
        "out = test_layer([Input(shape=(None, 1)), Input(shape=(None,)), Input(shape=(None,)), \n",
        "                  Input(shape=(None,)), Input(shape=(None, 1))])\n",
        "assert out.get_shape().as_list() == [None, None, 32]\n",
        "\n",
        "out = test_layer([Input(shape=(16, 1)), Input(shape=(16,)), Input(shape=(16,)), \n",
        "                  Input(shape=(16,)), Input(shape=(16, 1))])\n",
        "assert out.get_shape().as_list() == [None, 16, 32]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7IGhN8jNFrB",
        "colab_type": "text"
      },
      "source": [
        "### Class Encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6mNQeONNjSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, stacked_units, kernel_mn, recurrent_mn, dropout, **kwargs):\n",
        "    super(Encoder, self).__init__(**kwargs)\n",
        "    self.stacked_units = stacked_units\n",
        "    self.kernel_mn = kernel_mn\n",
        "    self.recurrent_mn = recurrent_mn\n",
        "    self.dropout = dropout\n",
        "\n",
        "    # for loop to stack lstms, with returning sequences\n",
        "    self.lstms = self.stack_lstms()\n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    for lstm in self.lstms[:-1]:\n",
        "      inputs = lstm(inputs, training=training)\n",
        "    encoder_outputs, state_h, state_c = self.lstms[-1](inputs ,training=training)\n",
        "    encoder_states = [state_h, state_c]  \n",
        "    return encoder_outputs, encoder_states\n",
        "\n",
        "  def stack_lstms(self):\n",
        "    # for loop to stack lstms, with returning sequences\n",
        "    lstms = []\n",
        "    for units in self.stacked_units[:-1]:\n",
        "      lstm_ = LSTM(\n",
        "        units, \n",
        "        return_sequences=True, \n",
        "        kernel_constraint=max_norm(self.kernel_mn),\n",
        "        recurrent_constraint=max_norm(self.recurrent_mn), \n",
        "        dropout=self.dropout)\n",
        "      lstms.append(lstm_)\n",
        "\n",
        "    # the last lstm returns the states as well as the sequences\n",
        "    lstm_ = LSTM(\n",
        "      self.stacked_units[-1], \n",
        "      return_state=True, \n",
        "      return_sequences=True, \n",
        "      kernel_constraint=max_norm(self.kernel_mn),\n",
        "      recurrent_constraint=max_norm(self.recurrent_mn), \n",
        "      dropout=self.dropout)\n",
        "    lstms.append(lstm_)\n",
        "    return lstms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgi6D18dVW5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# simple testing\n",
        "input_test_layer = ProcessInput(\n",
        "  dense_dim=None, dense_activation=None, gaussian_noise=0.0, num_ids=7, \n",
        "  day_of_week_dim=10, hour_of_day_dim=10, patient_id_dim=10)\n",
        "\n",
        "input_test_layer = input_test_layer(\n",
        "  [Input(shape=(16, 1)), Input(shape=(16,)), Input(shape=(16,)), \n",
        "   Input(shape=(16,)), Input(shape=(16, 1))])\n",
        "\n",
        "test_enc = Encoder(stacked_units=[5, 5], kernel_mn=0.3, recurrent_mn=0.1, dropout=0.3)\n",
        "enc_lstms = test_enc.lstms\n",
        "assert len(enc_lstms) == 2\n",
        "\n",
        "encoder_outputs, encoder_states = test_enc(input_test_layer)\n",
        "assert len(encoder_states) == 2\n",
        "assert encoder_outputs.get_shape().as_list() == [None, 16, 5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOTtCAOvfUVm",
        "colab_type": "text"
      },
      "source": [
        "### Attention Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd3XZqJ7W9al",
        "colab_type": "text"
      },
      "source": [
        "#### Additive Attention\n",
        "\n",
        "Bahdanau et al. https://arxiv.org/pdf/1409.0473.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnhhI_pkfizQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AdditiveAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, kernel_mn):\n",
        "    super(AdditiveAttention, self).__init__()\n",
        "    self.units = units\n",
        "    self.kernel_mn = kernel_mn\n",
        "    \n",
        "    # Weights relating to target states (decoder typically)\n",
        "    self.source_weight = tf.keras.layers.Dense(units, kernel_constraint=max_norm(kernel_mn))  \n",
        "    # Weights relating to source states (encoder typically)\n",
        "    self.target_weight = tf.keras.layers.Dense(units, kernel_constraint=max_norm(kernel_mn))  \n",
        "    # score transformation\n",
        "    self.score_weight = tf.keras.layers.Dense(1)  \n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    # target is also referred as query, and source as values.\n",
        "    target, source = inputs\n",
        "    # target must be a hidden vector of shape=(None, hidden_size), \n",
        "    # while source has shape=(None, steps, hidden_size)\n",
        "\n",
        "    # expand the dimension of the target, (None, 1, hidden_size)\n",
        "    target_with_time_axis = tf.expand_dims(target, axis=1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.score_weight(\n",
        "        tf.nn.tanh(\n",
        "            self.source_weight(source) + self.target_weight(target_with_time_axis)\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * source\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei9BDIdLfVgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# simple testing\n",
        "input_test_layer = ProcessInput(\n",
        "  dense_dim=None, dense_activation=None, gaussian_noise=0.0, num_ids=7, \n",
        "  day_of_week_dim=10, hour_of_day_dim=10, patient_id_dim=10)\n",
        "\n",
        "input_test_layer = input_test_layer(\n",
        "  [Input(shape=(16, 1)), Input(shape=(16,)), Input(shape=(16,)), \n",
        "   Input(shape=(16,)), Input(shape=(16, 1))])\n",
        "\n",
        "test_enc = Encoder(stacked_units=[5, 5], kernel_mn=0.3, recurrent_mn=0.1, dropout=0.3)\n",
        "encoder_outputs, encoder_states = test_enc(input_test_layer)\n",
        "encoder_hidden = encoder_states[0]\n",
        "\n",
        "attention_layer = AdditiveAttention(10, kernel_mn=0.2)\n",
        "attention_result, attention_weights = attention_layer([encoder_hidden, encoder_outputs])\n",
        "\n",
        "assert attention_layer.source_weight.get_input_shape_at(0) == (None, 16, 5)\n",
        "assert attention_layer.source_weight.get_output_shape_at(0) == (None, 16, 10)\n",
        "\n",
        "# why this does not work?\n",
        "# attention_layer.score_weight.get_output_shape_at(0)\n",
        "\n",
        "assert attention_result.get_shape()[1] == 5\n",
        "assert attention_weights.get_shape().as_list() == [None, 16, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJL0koKjXFtH",
        "colab_type": "text"
      },
      "source": [
        "#### General Attention\n",
        "\n",
        "Luong et al., https://arxiv.org/pdf/1508.04025.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-I84JZu6XZOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GeneralAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, kernel_mn):\n",
        "    super(GeneralAttention, self).__init__()\n",
        "    self.units = units\n",
        "    self.kernel_mn = kernel_mn\n",
        "    \n",
        "    self.score_weight = tf.keras.layers.Dense(units, kernel_constraint=max_norm(kernel_mn))  \n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    # target is also referred as query, and source as values.\n",
        "    target, source = inputs\n",
        "    # target must be a hidden vector of shape=(None, hidden_size), \n",
        "    # while source has shape=(None, steps, hidden_size)\n",
        "\n",
        "    # expand the dimension of the target, (None, 1, hidden_size)\n",
        "    target_with_time_axis = tf.expand_dims(target, axis=1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are performing the dot product along that dim\n",
        "    score = tf.matmul(a=target_with_time_axis, b=self.score_weight(source), transpose_b=True)\n",
        "    score = tf.transpose(score, perm=[0, 2, 1])\n",
        "    \n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * source\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiJ64DUlfWSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# simple testing\n",
        "input_test_layer = ProcessInput(\n",
        "  dense_dim=None, dense_activation=None, gaussian_noise=0.0, num_ids=7, \n",
        "  day_of_week_dim=10, hour_of_day_dim=10, patient_id_dim=10)\n",
        "\n",
        "input_test_layer = input_test_layer(\n",
        "  [Input(shape=(16, 1)), Input(shape=(16,)), Input(shape=(16,)), \n",
        "   Input(shape=(16,)), Input(shape=(16, 1))])\n",
        "\n",
        "test_enc = Encoder(stacked_units=[5, 5], kernel_mn=0.3, recurrent_mn=0.1, dropout=0.3)\n",
        "encoder_outputs, encoder_states = test_enc(input_test_layer)\n",
        "encoder_hidden = encoder_states[0]\n",
        "\n",
        "attention_layer = GeneralAttention(5, kernel_mn=0.2)\n",
        "attention_result, attention_weights = attention_layer([encoder_hidden, encoder_outputs])\n",
        "\n",
        "assert attention_layer.score_weight.get_input_shape_at(0) == (None, 16, 5)\n",
        "\n",
        "# why this does not work?\n",
        "# attention_layer.score_weight.get_output_shape_at(0)\n",
        "\n",
        "assert attention_result.get_shape()[1] == 5\n",
        "assert attention_weights.get_shape().as_list() == [None, 16, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTykdzX0edic",
        "colab_type": "text"
      },
      "source": [
        "#### Scaled dot-product attention\n",
        "\n",
        "Vaswani et al., http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFs2VZCQeq6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ScaledDotProductAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(ScaledDotProductAttention, self).__init__()\n",
        "    self.units = units\n",
        "    self.square_root = tf.math.sqrt(tf.cast(self.units,tf.float32))\n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    # target is also referred as query, and source as values.\n",
        "    target, source = inputs\n",
        "    # target must be a hidden vector of shape=(None, hidden_size), \n",
        "    # while source has shape=(None, steps, hidden_size)\n",
        "\n",
        "    # expand the dimension of the target, (None, 1, hidden_size)\n",
        "    target_with_time_axis = tf.expand_dims(target, axis=1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are performing the dot product along that dim\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are performing the dot product along that dim\n",
        "    score = tf.matmul(a=target_with_time_axis, b=source, transpose_b=True)\n",
        "    score = tf.divide(score, self.square_root)\n",
        "    score = tf.transpose(score, perm=[0, 2, 1])\n",
        "    \n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * source\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M3uRLsT3oBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# simple testing\n",
        "input_test_layer = ProcessInput(\n",
        "  dense_dim=None, dense_activation=None, gaussian_noise=0.0, num_ids=7, \n",
        "  day_of_week_dim=10, hour_of_day_dim=10, patient_id_dim=10)\n",
        "\n",
        "input_test_layer = input_test_layer(\n",
        "  [Input(shape=(16, 1)), Input(shape=(16,)), Input(shape=(16,)), \n",
        "   Input(shape=(16,)), Input(shape=(16, 1))])\n",
        "\n",
        "test_enc = Encoder(stacked_units=[5, 5], kernel_mn=0.3, recurrent_mn=0.1, dropout=0.3)\n",
        "encoder_outputs, encoder_states = test_enc(input_test_layer)\n",
        "encoder_hidden = encoder_states[0]\n",
        "\n",
        "attention_layer = GeneralAttention(5, kernel_mn=0.2)\n",
        "attention_result, attention_weights = attention_layer([encoder_hidden, encoder_outputs])\n",
        "\n",
        "assert attention_layer.score_weight.get_input_shape_at(0) == (None, 16, 5)\n",
        "\n",
        "# why this does not work?\n",
        "# attention_layer.score_weight.get_output_shape_at(0)\n",
        "\n",
        "assert attention_result.get_shape()[1] == 5\n",
        "assert attention_weights.get_shape().as_list() == [None, 16, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAq_Rd-FfenX",
        "colab_type": "text"
      },
      "source": [
        "### Class decoder\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu6pRZ0dfjXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(Encoder):\n",
        "  def __init__(self, stacked_units, kernel_mn, recurrent_mn, dropout, \n",
        "               mlp_units, activation_mlp, mlp_mn, use_attention, attention_score,\n",
        "               attention_kernel_mn, **kwargs):\n",
        "    super(Decoder, self).__init__(stacked_units, kernel_mn, recurrent_mn, dropout, **kwargs)\n",
        "    self.mlp_units = mlp_units \n",
        "    self.activation_mlp = activation_mlp\n",
        "    self.mlp_mn = mlp_mn\n",
        "    self.use_attention = use_attention\n",
        "    self.attention_score = attention_score.lower()\n",
        "    self.attention_kernel_mn = attention_kernel_mn\n",
        "\n",
        "    self.mlp = self.mlp()\n",
        "    if self.use_attention:\n",
        "      if self.attention_score == 'additive':\n",
        "        self.attention = AdditiveAttention(units=self.stacked_units[-1], \n",
        "                                          kernel_mn=attention_kernel_mn)\n",
        "      elif self.attention_score == 'general':\n",
        "        self.attention = GeneralAttention(units=self.stacked_units[-1], \n",
        "                                          kernel_mn=attention_kernel_mn)\n",
        "      elif self.attention_score == 'dot-product':\n",
        "        self.attention = ScaledDotProductAttention(units=self.stacked_units[-1])\n",
        "      else:\n",
        "        raise NotImplementedError(\"Attention score %s not available\" %self.attention_score)\n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    # split inputs in teacher forcing input (can be None), \n",
        "    # target states (list of decoder states at t-1),\n",
        "    # source states (outputs from the encoder)\n",
        "    # and context_vector from previous step (can be None)\n",
        "    teacher_input, target_states_pre, source_seq, context_vector_pre = inputs\n",
        "    \n",
        "    # concatenate inputs from encoder and previous step (teacher forcing, context_vector)\n",
        "    state_vector = context_vector_pre if self.use_attention else source_seq[:, -1, :]\n",
        "    state_vector = tf.expand_dims(state_vector, 1)  # shape=(None, 1, dec_hidden_size)\n",
        "    if teacher_input is not None:\n",
        "      merged = tf.concat([state_vector, teacher_input], axis=-1)\n",
        "    else:\n",
        "      merged = state_vector\n",
        "    \n",
        "    # pass the merged vector through the stack of LSTMs\n",
        "    # Initialize LSTM cell with target states at step t-1\n",
        "    for lstm in self.lstms[:-1]:\n",
        "      merged = lstm(merged, training=training, initial_state=target_states_pre)\n",
        "    decoder_outputs, state_h, state_c = self.lstms[-1](merged, training=training, \n",
        "                                                       initial_state=target_states_pre)\n",
        "    decoder_states = [state_h, state_c]  \n",
        "    # reshape output, as we pass one step at a time\n",
        "    decoder_output = tf.reshape(decoder_outputs, (-1, decoder_outputs.shape[2]))\n",
        "    \n",
        "    # Attention mechanism for current step\n",
        "    if self.use_attention:\n",
        "      target_state = decoder_states[0]\n",
        "      context_vector, attention_weights = self.attention(inputs=[target_state, source_seq], \n",
        "                                                         training=training)\n",
        "      concat = tf.concat([context_vector, decoder_output], axis=-1)\n",
        "    else:\n",
        "      concat = decoder_output\n",
        "      context_vector = None\n",
        "\n",
        "    # pass the output to the mlp. output shape=(None, 1)\n",
        "    out = concat\n",
        "    for dense in self.mlp:\n",
        "      out = dense(out)\n",
        "    return out, decoder_states, context_vector\n",
        "\n",
        "  def mlp(self):\n",
        "    # MLP for output sequences\n",
        "    mlp_stack = []\n",
        "    for neurons in self.mlp_units:\n",
        "        decoder_dense = Dense(neurons, activation=self.activation_mlp, \n",
        "                              kernel_constraint=max_norm(self.mlp_mn))\n",
        "        mlp_stack.append(decoder_dense)\n",
        "    return mlp_stack"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzBo7FSFdGB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# simple testing\n",
        "input_test_layer = ProcessInput(\n",
        "  dense_dim=None, dense_activation=None, gaussian_noise=0.0, num_ids=7, \n",
        "  day_of_week_dim=10, hour_of_day_dim=10, patient_id_dim=10)\n",
        "\n",
        "input_test_layer = input_test_layer(\n",
        "  [Input(shape=(16, 1)), Input(shape=(16,)), Input(shape=(16,)), \n",
        "   Input(shape=(16,)), Input(shape=(16, 1))])\n",
        "\n",
        "test_enc = Encoder(stacked_units=[5, 5], kernel_mn=0.3, recurrent_mn=0.1, dropout=0.3)\n",
        "encoder_outputs, encoder_states = test_enc(input_test_layer)\n",
        "\n",
        "# test with teacher-forcing\n",
        "test_dec = Decoder(stacked_units=[5, 5], kernel_mn=0.3, recurrent_mn=0.1, \n",
        "                   dropout=0.3, mlp_units=[4, 1], activation_mlp='relu', \n",
        "                   mlp_mn=0.4, use_attention=True, attention_score='dot-product',\n",
        "                   attention_kernel_mn=0.2)\n",
        "input_teacher = Input(shape=(1, 1))\n",
        "\n",
        "# initial step\n",
        "# apply attention to the encoder\n",
        "context_vector_enc, attention_weights = test_dec.attention([encoder_states[0], encoder_outputs])\n",
        "\n",
        "input_dec = [input_teacher, encoder_states, encoder_outputs, context_vector_enc]\n",
        "decoder_outputs, decoder_states, context_vector = test_dec(input_dec)\n",
        "\n",
        "assert len(decoder_states) == 2\n",
        "assert decoder_states[0].get_shape().as_list() == [None, 5]\n",
        "assert decoder_outputs.get_shape().as_list() == [None, 1]\n",
        "assert context_vector.get_shape().as_list() == [None, 5]\n",
        "\n",
        "# subsequent steps. \n",
        "input_dec = [input_teacher, decoder_states, encoder_outputs, context_vector]\n",
        "decoder_outputs, decoder_states, context_vector = test_dec(input_dec)\n",
        "\n",
        "assert decoder_outputs.get_shape().as_list() == [None, 1]\n",
        "assert context_vector.get_shape().as_list() == [None, 5]\n",
        "\n",
        "# Finally test without teacher-forcing\n",
        "test_dec = Decoder(stacked_units=[5, 5], kernel_mn=0.3, recurrent_mn=0.1, \n",
        "                   dropout=0.3, mlp_units=[4, 1], activation_mlp='relu', \n",
        "                   mlp_mn=0.4, use_attention=True, attention_score='general',\n",
        "                   attention_kernel_mn=0.2)\n",
        "input_teacher = None\n",
        "\n",
        "# initial step\n",
        "# apply attention to the encoder\n",
        "context_vector_enc, attention_weights = test_dec.attention([encoder_states[0], encoder_outputs])\n",
        "\n",
        "input_dec = [input_teacher, encoder_states, encoder_outputs, context_vector_enc]\n",
        "decoder_outputs, decoder_states, context_vector = test_dec(input_dec)\n",
        "\n",
        "assert len(decoder_states) == 2\n",
        "assert decoder_states[0].get_shape().as_list() == [None, 5]\n",
        "assert decoder_outputs.get_shape().as_list() == [None, 1]\n",
        "assert context_vector.get_shape().as_list() == [None, 5]\n",
        "\n",
        "# subsequent steps. \n",
        "input_dec = [input_teacher, decoder_states, encoder_outputs, context_vector]\n",
        "decoder_outputs, decoder_states, context_vector = test_dec(input_dec)\n",
        "assert decoder_outputs.get_shape().as_list() == [None, 1]\n",
        "assert context_vector.get_shape().as_list() == [None, 5]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q7AaPcRuNgM",
        "colab_type": "text"
      },
      "source": [
        "### Complete model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX5IgZ3vjd_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2SeqModel(tf.keras.Model):\n",
        "  def __init__(self, dense_dim, dense_activation, gaussian_noise,\n",
        "               num_ids, day_of_week_dim, hour_of_day_dim, patient_id_dim,\n",
        "               enc_stacked_units, enc_kernel_mn, enc_recurrent_mn,\n",
        "               enc_dropout, dec_stacked_units, dec_kernel_mn, dec_recurrent_mn, \n",
        "               dec_dropout, use_attention, attention_score, attention_kernel_mn, \n",
        "               mlp_units, activation_mlp, mlp_mn, \n",
        "               teacher_forcing, bernouilli_proba, start_char, \n",
        "               learning_rate, clipvalue, \n",
        "               **kwargs):\n",
        "    super(Seq2SeqModel, self).__init__(**kwargs)\n",
        "\n",
        "    # clear worksapce\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    # ProcessInput parameters\n",
        "    self.dense_dim = dense_dim \n",
        "    self.dense_activation = dense_activation \n",
        "    self.gaussian_noise = gaussian_noise\n",
        "    self.num_ids = num_ids \n",
        "    self.day_of_week_dim = day_of_week_dim \n",
        "    self.hour_of_day_dim = hour_of_day_dim \n",
        "    self.patient_id_dim = patient_id_dim\n",
        "\n",
        "    # Encoder Parameters\n",
        "    self.enc_stacked_units = enc_stacked_units \n",
        "    self.enc_kernel_mn = enc_kernel_mn \n",
        "    self.enc_recurrent_mn = enc_recurrent_mn\n",
        "    self.enc_dropout = enc_dropout \n",
        "\n",
        "    # Decoder parameters\n",
        "    self.dec_stacked_units = dec_stacked_units \n",
        "    self.dec_kernel_mn = dec_kernel_mn \n",
        "    self.dec_recurrent_mn = dec_recurrent_mn \n",
        "    self.dec_dropout = dec_dropout \n",
        "    self.mlp_units = mlp_units \n",
        "    self.activation_mlp = activation_mlp \n",
        "    self.mlp_mn = mlp_mn \n",
        "    # attention params\n",
        "    self.use_attention = use_attention\n",
        "    self.attention_score = attention_score\n",
        "    self.attention_kernel_mn = attention_kernel_mn\n",
        "\n",
        "    # Optimization parameters\n",
        "    self.learning_rate = learning_rate \n",
        "    self.clipvalue = clipvalue\n",
        "\n",
        "    # Others\n",
        "    self.teacher_forcing = teacher_forcing\n",
        "    self.bernouilli_proba = bernouilli_proba\n",
        "    self.start_char = start_char\n",
        "\n",
        "    # build and compile\n",
        "    self.build()\n",
        "    self.compile()\n",
        "\n",
        "  def build(self):\n",
        "    self.process_layer = ProcessInput(\n",
        "      dense_dim=self.dense_dim, dense_activation=self.dense_activation, \n",
        "      gaussian_noise=self.gaussian_noise, num_ids=self.num_ids, \n",
        "      day_of_week_dim=self.day_of_week_dim, hour_of_day_dim=self.hour_of_day_dim, \n",
        "      patient_id_dim=self.patient_id_dim)\n",
        "\n",
        "    self.encoder = Encoder(\n",
        "      stacked_units=self.enc_stacked_units, kernel_mn=self.enc_kernel_mn, \n",
        "      recurrent_mn=self.enc_recurrent_mn, dropout=self.enc_dropout)\n",
        "\n",
        "    self.decoder = Decoder(\n",
        "      stacked_units=self.dec_stacked_units, kernel_mn=self.dec_kernel_mn, \n",
        "      recurrent_mn=self.dec_recurrent_mn, dropout=self.dec_dropout, \n",
        "      mlp_units=self.mlp_units, activation_mlp=self.activation_mlp, \n",
        "      mlp_mn=self.mlp_mn, use_attention=self.use_attention, \n",
        "      attention_score=self.attention_score, attention_kernel_mn=self.attention_kernel_mn)\n",
        "\n",
        "  def compile(self):\n",
        "    self.optimizer = tf.keras.optimizers.Adam(\n",
        "      learning_rate=self.learning_rate, clipvalue=self.clipvalue)\n",
        "    self.loss_function = tf.keras.losses.MeanSquaredError(name='mse')\n",
        "    # we use metrcis in train and eval since MAPE cannot be aplied properly \n",
        "    # unless values are in its original scaled. \n",
        "    self.metric_train = [tf.keras.metrics.MeanAbsoluteError(name='mae')]\n",
        "    self.metric_eval = [tf.keras.metrics.MeanAbsoluteError(name='mae'), \n",
        "                        tf.keras.metrics.MeanAbsolutePercentageError(name='mape')]\n",
        "  @tf.function\n",
        "  def train_step(self, inp, tar):\n",
        "    loss = 0\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "      input_enc = self.process_layer(inp, training=True)\n",
        "      enc_outputs, enc_states = self.encoder(input_enc, training=True)\n",
        "      dec_states = enc_states\n",
        "\n",
        "      # calculate context vector for last encoder state      \n",
        "      if self.use_attention:\n",
        "        context_vector, _ = self.decoder.attention([enc_states[0], enc_outputs])\n",
        "      else:\n",
        "        context_vector = None\n",
        "\n",
        "      # provide last time step in encoder as first input char\n",
        "      dec_input = tf.expand_dims(input_enc[:, -1, -1], axis=-1)\n",
        "      \n",
        "      for t in range(0, tar.shape[1]):\n",
        "        # expand dim of dec_input\n",
        "        if self.teacher_forcing:\n",
        "          dec_input = tf.expand_dims(dec_input, axis=1)\n",
        "        else:\n",
        "          dec_input = None\n",
        "\n",
        "        # passing enc_output to the decoder\n",
        "        inputs = [dec_input, dec_states, enc_outputs, context_vector]\n",
        "        predictions, dec_states, context_vector = self.decoder(inputs=inputs, training=True)\n",
        "\n",
        "        loss += self.loss_function(tar[:, t, :], predictions)\n",
        "        _ = [m.update_state(tar[:, t, :], predictions) for m in self.metric_train]\n",
        "          \n",
        "        # teacher-forcing with bernouilli distribution\n",
        "        if self.teacher_forcing:\n",
        "          p = tf.keras.backend.random_binomial(shape=(), p=self.bernouilli_proba)\n",
        "          dec_input = tf.keras.backend.switch(p<0.5, tar[:, t, :], predictions)\n",
        "    \n",
        "    batch_loss = loss / int(tar.shape[1])\n",
        "    variables = self.process_layer.trainable_variables + \\\n",
        "      self.encoder.trainable_variables + \\\n",
        "      self.decoder.trainable_variables \n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss\n",
        "\n",
        "  @tf.function\n",
        "  def inference_step(self, inp, steps, batch_size):\n",
        "\n",
        "    # initialize output tensor\n",
        "    prediction = []\n",
        "\n",
        "    # forward pass through process_layer and encoder\n",
        "    input_enc = self.process_layer(inp, training=False)\n",
        "    enc_outputs, enc_states = self.encoder(input_enc, training=False)\n",
        "    dec_states = enc_states\n",
        "    \n",
        "    # calculate context vector for last encoder state      \n",
        "    if self.use_attention:\n",
        "      context_vector, _ = self.decoder.attention([enc_states[0], enc_outputs])\n",
        "    else:\n",
        "      context_vector = None\n",
        "\n",
        "    # provide last time step in encoder as first input char\n",
        "    dec_input = tf.expand_dims(input_enc[:, -1, -1], axis=-1)\n",
        "    \n",
        "    # forward pass through the decoder\n",
        "    for t in range(0, steps):\n",
        "      # expand dim of dec_input\n",
        "      if self.teacher_forcing:\n",
        "        dec_input = tf.expand_dims(dec_input, axis=1)\n",
        "      else:\n",
        "        dec_input = None\n",
        "      # passing enc_output to the decoder\n",
        "      inputs = [dec_input, dec_states, enc_outputs, context_vector]\n",
        "      step_prediction, dec_states, context_vector = self.decoder(inputs=inputs, \n",
        "                                                                 training=False)\n",
        "      dec_input = step_prediction\n",
        "      prediction.append(tf.expand_dims(dec_input, axis=1))\n",
        "    \n",
        "    # concat predictions\n",
        "    prediction = tf.concat(prediction, axis=1)\n",
        "    \n",
        "    return prediction\n",
        "\n",
        "  def fit_custom(self, train_dataset, epochs, vad_dataset, steps_per_epoch=None):\n",
        "    history = {}\n",
        "    history['loss'] = []\n",
        "    history['val_loss'] = []\n",
        "    for m in self.metric_train:\n",
        "      history[m.name] = []  \n",
        "      history['val_%s' % (m.name)] = []  \n",
        "\n",
        "    # steps per epoch in train and validation\n",
        "    if steps_per_epoch is None:\n",
        "      len_train = tf.data.experimental.cardinality(train_dataset).numpy()\n",
        "    else: \n",
        "      len_train = steps_per_epoch\n",
        "    len_vad = tf.data.experimental.cardinality(vad_dataset).numpy()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "    \n",
        "      start = time.time()\n",
        "      print('Epoch ({}/{})'.format(epoch+1, epochs))\n",
        "      \n",
        "      pbar = tf.keras.utils.Progbar(len_train + len_vad)\n",
        "\n",
        "      # training loop\n",
        "      loop = enumerate(train_dataset.take(len_train))\n",
        "\n",
        "      total_loss = 0\n",
        "\n",
        "      for (batch, (inp, targ, scaler)) in loop:\n",
        "        batch_loss = self.train_step(inp, targ)\n",
        "        total_loss += batch_loss\n",
        "        total_metric = [(m.name, m.result()) for m in self.metric_train]\n",
        "        pbar.update(batch, values=[(\"loss\", batch_loss)] + total_metric)\n",
        "      \n",
        "      total_loss  /= float(len_train)\n",
        "      history['loss'].append(total_loss.numpy())\n",
        "      for m in self.metric_train:\n",
        "        history[m.name].append(m.result().numpy())\n",
        "        m.reset_states()\n",
        "\n",
        "      # validation loop (inference and eval)\n",
        "      loop = enumerate(vad_dataset)\n",
        "      \n",
        "      # number of steps\n",
        "      batch_size, steps = [(tar.shape[0], tar.shape[1]) for inp, tar, scaler in vad_dataset.take(1)][0]\n",
        "      total_loss = 0\n",
        "      \n",
        "      for (batch, (inp, targ, _)) in loop:\n",
        "        prediction = self.inference_step(inp, steps, batch_size)\n",
        "        batch_loss = self.loss_function(prediction, targ)\n",
        "        total_loss += batch_loss\n",
        "        # update metrics\n",
        "        [m.update_state(targ, prediction) for m in self.metric_train]\n",
        "        total_metric = [('val_%s' %(m.name), m.result()) for m in self.metric_train]\n",
        "        # update progress bar\n",
        "        pbar.update(batch, values=[(\"val_loss\", batch_loss)] + total_metric)  \n",
        "      \n",
        "      total_loss  /= float(len_vad)\n",
        "      history['val_loss'].append(total_loss.numpy())\n",
        "      for m in self.metric_train:\n",
        "        history['val_%s' %(m.name)].append(m.result().numpy())\n",
        "        m.reset_states()\n",
        "\n",
        "      print(' ({:.0f} sec)\\n'.format( time.time() - start))\n",
        "    return history\n",
        "\n",
        "  @staticmethod\n",
        "  def rescale_batch(y, scale):\n",
        "    # shape(y) = (batch_size, steps, 1)\n",
        "    # shape(scale) = (batch_size, 2, 1, 1)\n",
        "    slope = scale[:, 1, :, :] - scale[:, 0, :, :]\n",
        "    bias = scale[:, 0, :, :]\n",
        "    return tf.math.multiply(y, slope) + bias\n",
        "\n",
        "  def predict_evaluate_custom(self, dataset, label='test', rescale=True):\n",
        "    start = time.time()\n",
        "\n",
        "    # inference loop\n",
        "    loop = enumerate(dataset)\n",
        "\n",
        "    # number of steps (we do not perform inference for the EOS)\n",
        "    batch_size, steps = [(tar.shape[0], tar.shape[1]) for inp, tar, _ in dataset.take(1)][0]\n",
        "\n",
        "    # num steps and progress bar    \n",
        "    len_ = tf.data.experimental.cardinality(dataset).numpy()\n",
        "    pbar = tf.keras.utils.Progbar(len_)\n",
        "\n",
        "    prediction = []\n",
        "    for (batch, (inp, targ, scaler)) in loop:\n",
        "      # prediction has shape=(batch_size, steps, 1)\n",
        "      p = self.inference_step(inp, steps, batch_size)\n",
        "      if rescale:\n",
        "        # scale target and predicition so as to get the real mean absolute error\n",
        "        targ = self.rescale_batch(scale=scaler, y=targ)\n",
        "        p = self.rescale_batch(scale=scaler, y=p)\n",
        "        \n",
        "      # update metrics\n",
        "      [m.update_state(targ, p) for m in self.metric_eval]\n",
        "      total_metric = [('%s_%s' %(label, m.name), m.result()) for m in self.metric_eval]\n",
        "      # update progress bar\n",
        "      pbar.update(batch, values=total_metric)  \n",
        "      # update total predictions\n",
        "      prediction.append(p)\n",
        "    \n",
        "    prediction = tf.concat(prediction, axis=0).numpy()\n",
        "    total_metric = [('%s_%s' %(label, m.name), m.result()) for m in self.metric_eval]\n",
        "    [m.reset_states() for m in self.metric_eval]\n",
        "    print(' ({:.0f} sec)\\n'.format( time.time() - start))\n",
        "    return prediction, total_metric\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEFXP_uZ7uv5",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSJtqzkmDsvR",
        "colab_type": "text"
      },
      "source": [
        "### Standard params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qZ0rWRdmwXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "standard_inputs = dict(\n",
        "  # ProcessInput params\n",
        "  dense_dim = 80,\n",
        "  dense_activation = 'elu',\n",
        "  gaussian_noise = 0.0,\n",
        "  num_ids = num_identifiers,\n",
        "  day_of_week_dim = 20,\n",
        "  hour_of_day_dim = 20,\n",
        "  patient_id_dim = 20,\n",
        "  # Encoder params\n",
        "  enc_stacked_units = [128],\n",
        "  enc_kernel_mn = 0.1,\n",
        "  enc_recurrent_mn = 0.1,\n",
        "  enc_dropout = 0.0,\n",
        "  # Decoder params\n",
        "  dec_stacked_units = [128], \n",
        "  dec_kernel_mn = 0.1,\n",
        "  dec_recurrent_mn = 0.1,\n",
        "  dec_dropout = 0.0,\n",
        "  mlp_units = [64, 32, 16, 8, 1],\n",
        "  activation_mlp = 'elu',\n",
        "  mlp_mn = 0.2,\n",
        "  # attention params\n",
        "  use_attention = True,\n",
        "  attention_kernel_mn = 0.1,\n",
        "  attention_score = 'dot-product',\n",
        "  # teacher forcing \n",
        "  bernouilli_proba = 1.0,\n",
        "  teacher_forcing = None,\n",
        "  start_char = -1.0,\n",
        "  # Optimization params\n",
        "  learning_rate = 5e-4,\n",
        "  clipvalue = 0.5\n",
        ")\n",
        "\n",
        "epochs = 40"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doxM2wVg0CTc",
        "colab_type": "text"
      },
      "source": [
        "### Train without attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn8EJPsF0BQI",
        "colab_type": "code",
        "outputId": "bad03d35-ce4c-4dfd-da29-180511e2fa39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "inputs = copy.deepcopy(standard_inputs)\n",
        "inputs['teacher_forcing'] = False\n",
        "inputs['use_attention'] = False\n",
        "inputs['dense_dim'] = None\n",
        "\n",
        "model = Seq2SeqModel(**inputs)\n",
        "h = model.fit_custom(train_dataset=train_dataset, epochs=epochs, vad_dataset=vad_dataset)\n",
        "\n",
        "plot_train_history(h, \"Loss %s\" %(model.loss_function), 'loss')\n",
        "for metric in model.metric_train:\n",
        "  plot_train_history(h, 'Metric %s' %(metric.name), metric.name)\n",
        "\n",
        "prediction_vad, metrics_vad = model.predict_evaluate_custom(vad_dataset, \n",
        "                                                            label='vad', \n",
        "                                                            rescale=True)\n",
        "for label, m in metrics_vad:\n",
        "  print(label, m.numpy())\n",
        "\n",
        "for _ in np.arange(10):\n",
        "  plot_results(vad, prediction_vad, history, future)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch (1/40)\n",
            " 201/1919 [==>...........................] - ETA: 18:41 - loss: 0.0417 - mae: 0.1687 - val_loss: 0.0389 - val_mae: 0.1529 (131 sec)\n",
            "\n",
            "Epoch (2/40)\n",
            " 200/1919 [==>...........................] - ETA: 15:19 - loss: 0.0372 - mae: 0.1474 - val_loss: 0.0357 - val_mae: 0.1491 (107 sec)\n",
            "\n",
            "Epoch (3/40)\n",
            " 200/1919 [==>...........................] - ETA: 15:24 - loss: 0.0355 - mae: 0.1420 - val_loss: 0.0353 - val_mae: 0.1513 (108 sec)\n",
            "\n",
            "Epoch (4/40)\n",
            " 201/1919 [==>...........................] - ETA: 15:21 - loss: 0.0345 - mae: 0.1392 - val_loss: 0.0336 - val_mae: 0.1441 (108 sec)\n",
            "\n",
            "Epoch (5/40)\n",
            " 200/1919 [==>...........................] - ETA: 15:22 - loss: 0.0338 - mae: 0.1374 - val_loss: 0.0342 - val_mae: 0.1476 (107 sec)\n",
            "\n",
            "Epoch (6/40)\n",
            " 200/1919 [==>...........................] - ETA: 15:18 - loss: 0.0333 - mae: 0.1355 - val_loss: 0.0324 - val_mae: 0.1390 (107 sec)\n",
            "\n",
            "Epoch (7/40)\n",
            " 201/1919 [==>...........................] - ETA: 15:05 - loss: 0.0328 - mae: 0.1342 - val_loss: 0.0332 - val_mae: 0.1380 (106 sec)\n",
            "\n",
            "Epoch (8/40)\n",
            " 200/1919 [==>...........................] - ETA: 15:19 - loss: 0.0324 - mae: 0.1330 - val_loss: 0.0327 - val_mae: 0.1423 (107 sec)\n",
            "\n",
            "Epoch (9/40)\n",
            " 200/1919 [==>...........................] - ETA: 15:22 - loss: 0.0321 - mae: 0.1324 - val_loss: 0.0321 - val_mae: 0.1379 (107 sec)\n",
            "\n",
            "Epoch (10/40)\n",
            " 200/1919 [==>...........................] - ETA: 15:22 - loss: 0.0319 - mae: 0.1318 - val_loss: 0.0325 - val_mae: 0.1392 (107 sec)\n",
            "\n",
            "Epoch (11/40)\n",
            " 200/1919 [==>...........................] - ETA: 15:16 - loss: 0.0318 - mae: 0.1317 - val_loss: 0.0321 - val_mae: 0.1370 (107 sec)\n",
            "\n",
            "Epoch (12/40)\n",
            " 200/1919 [==>...........................] - ETA: 15:27 - loss: 0.0316 - mae: 0.1309 - val_loss: 0.0318 - val_mae: 0.1372 (108 sec)\n",
            "\n",
            "Epoch (13/40)\n",
            " 200/1919 [==>...........................] - ETA: 15:28 - loss: 0.0315 - mae: 0.1303 - val_loss: 0.0316 - val_mae: 0.1382 (108 sec)\n",
            "\n",
            "Epoch (14/40)\n",
            " 200/1919 [==>...........................] - ETA: 15:20 - loss: 0.0313 - mae: 0.1301 - val_loss: 0.0312 - val_mae: 0.1356 (107 sec)\n",
            "\n",
            "Epoch (15/40)\n",
            " 200/1919 [==>...........................] - ETA: 15:13 - loss: 0.0313 - mae: 0.1301 - val_loss: 0.0317 - val_mae: 0.1350 (106 sec)\n",
            "\n",
            "Epoch (16/40)\n",
            " 200/1919 [==>...........................] - ETA: 15:21 - loss: 0.0312 - mae: 0.1296 - val_loss: 0.0315 - val_mae: 0.1351 (107 sec)\n",
            "\n",
            "Epoch (17/40)\n",
            " 200/1919 [==>...........................] - ETA: 15:12 - loss: 0.0311 - mae: 0.1294 - val_loss: 0.0314 - val_mae: 0.1375 (106 sec)\n",
            "\n",
            "Epoch (18/40)\n",
            " 200/1919 [==>...........................] - ETA: 15:14 - loss: 0.0310 - mae: 0.1291 - val_loss: 0.0308 - val_mae: 0.1332 (106 sec)\n",
            "\n",
            "Epoch (19/40)\n",
            " 200/1919 [==>...........................] - ETA: 15:20 - loss: 0.0310 - mae: 0.1290 - val_loss: 0.0312 - val_mae: 0.1329 (107 sec)\n",
            "\n",
            "Epoch (20/40)\n",
            " 200/1919 [==>...........................] - ETA: 15:12 - loss: 0.0310 - mae: 0.1290 - val_loss: 0.0310 - val_mae: 0.1367 (106 sec)\n",
            "\n",
            "Epoch (21/40)\n",
            " 200/1919 [==>...........................] - ETA: 15:13 - loss: 0.0310 - mae: 0.1288 - val_loss: 0.0308 - val_mae: 0.1340 (106 sec)\n",
            "\n",
            "Epoch (22/40)\n",
            " 200/1919 [==>...........................] - ETA: 15:30 - loss: 0.0308 - mae: 0.1286 - val_loss: 0.0310 - val_mae: 0.1350 (108 sec)\n",
            "\n",
            "Epoch (23/40)\n",
            " 201/1919 [==>...........................] - ETA: 14:59 - loss: 0.0308 - mae: 0.1289 - val_loss: 0.0308 - val_mae: 0.1331 (105 sec)\n",
            "\n",
            "Epoch (24/40)\n",
            " 200/1919 [==>...........................] - ETA: 15:15 - loss: 0.0308 - mae: 0.1282 - val_loss: 0.0307 - val_mae: 0.1328 (107 sec)\n",
            "\n",
            "Epoch (25/40)\n",
            " 201/1919 [==>...........................] - ETA: 15:15 - loss: 0.0307 - mae: 0.1280 - val_loss: 0.0313 - val_mae: 0.1369 (107 sec)\n",
            "\n",
            "Epoch (26/40)\n",
            " 200/1919 [==>...........................] - ETA: 15:10 - loss: 0.0307 - mae: 0.1281 - val_loss: 0.0313 - val_mae: 0.1362 (106 sec)\n",
            "\n",
            "Epoch (27/40)\n",
            " 200/1919 [==>...........................] - ETA: 15:14 - loss: 0.0307 - mae: 0.1282 - val_loss: 0.0310 - val_mae: 0.1344 (106 sec)\n",
            "\n",
            "Epoch (28/40)\n",
            " 200/1919 [==>...........................] - ETA: 15:10 - loss: 0.0307 - mae: 0.1281 - val_loss: 0.0305 - val_mae: 0.1323 (106 sec)\n",
            "\n",
            "Epoch (29/40)\n",
            " 201/1919 [==>...........................] - ETA: 15:04 - loss: 0.0307 - mae: 0.1279 - val_loss: 0.0309 - val_mae: 0.1351 (106 sec)\n",
            "\n",
            "Epoch (30/40)\n",
            " 200/1919 [==>...........................] - ETA: 15:10 - loss: 0.0306 - mae: 0.1279 - val_loss: 0.0311 - val_mae: 0.1326 (106 sec)\n",
            "\n",
            "Epoch (31/40)\n",
            " 200/1919 [==>...........................] - ETA: 15:17 - loss: 0.0306 - mae: 0.1276 - val_loss: 0.0305 - val_mae: 0.1314 (107 sec)\n",
            "\n",
            "Epoch (32/40)\n",
            " 200/1919 [==>...........................] - ETA: 15:18 - loss: 0.0306 - mae: 0.1277 - val_loss: 0.0309 - val_mae: 0.1344 (107 sec)\n",
            "\n",
            "Epoch (33/40)\n",
            "1560/1919 [=======================>......] - ETA: 21s - loss: 0.0306 - mae: 0.1277"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2CFuXapjiva",
        "colab_type": "text"
      },
      "source": [
        "### Teacher forcing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nSB7SSKhD0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = copy.deepcopy(standard_inputs)\n",
        "inputs['teacher_forcing'] = True\n",
        "inputs['bernouilli_proba'] = 1.0\n",
        "inputs['use_attention'] = False\n",
        "inputs['dense_dim'] = None\n",
        "\n",
        "model = Seq2SeqModel(**inputs)\n",
        "h = model.fit_custom(train_dataset=train_dataset, epochs=epochs, vad_dataset=vad_dataset)\n",
        "\n",
        "plot_train_history(h, \"Loss %s\" %(model.loss_function), 'loss')\n",
        "for metric in model.metric_train:\n",
        "  plot_train_history(h, 'Metric %s' %(metric.name), metric.name)\n",
        "\n",
        "prediction_vad, metrics_vad = model.predict_evaluate_custom(vad_dataset, \n",
        "                                                            label='vad', \n",
        "                                                            rescale=True)\n",
        "for label, m in metrics_vad:\n",
        "  print(label, m.numpy())\n",
        "\n",
        "for _ in np.arange(10):\n",
        "  plot_results(vad, prediction_vad, history, future)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBZex3MChFFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = copy.deepcopy(standard_inputs)\n",
        "inputs['teacher_forcing'] = True\n",
        "inputs['bernouilli_proba'] = 0.7\n",
        "inputs['use_attention'] = False\n",
        "inputs['dense_dim'] = None\n",
        "\n",
        "model = Seq2SeqModel(**inputs)\n",
        "h = model.fit_custom(train_dataset=train_dataset, epochs=epochs, vad_dataset=vad_dataset)\n",
        "\n",
        "plot_train_history(h, \"Loss %s\" %(model.loss_function), 'loss')\n",
        "for metric in model.metric_train:\n",
        "  plot_train_history(h, 'Metric %s' %(metric.name), metric.name)\n",
        "\n",
        "prediction_vad, metrics_vad = model.predict_evaluate_custom(vad_dataset, \n",
        "                                                            label='vad', \n",
        "                                                            rescale=True)\n",
        "for label, m in metrics_vad:\n",
        "  print(label, m.numpy())\n",
        "\n",
        "for _ in np.arange(10):\n",
        "  plot_results(vad, prediction_vad, history, future)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJXCpEMW0P-R",
        "colab_type": "text"
      },
      "source": [
        "### Train with attention\n",
        "\n",
        "Here we recommend using teacher forcing to speed up computation (models with attention take for ages!). Since during inference the model does't see true targets, we recommend setting the `bernouilli_proba` param above 0.5 (so that more predicitons than true targets are passed to the next time step while training)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNApZ87ftxPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = copy.deepcopy(standard_inputs)\n",
        "inputs['teacher_forcing'] = True\n",
        "inputs['bernouilli_proba'] = 0.9\n",
        "inputs['use_attention'] = True\n",
        "inputs['attention_score'] = 'additive'\n",
        "inputs['dense_dim'] = None\n",
        "\n",
        "model = Seq2SeqModel(**inputs)\n",
        "h = model.fit_custom(train_dataset=train_dataset, epochs=epochs, vad_dataset=vad_dataset)\n",
        "\n",
        "plot_train_history(h, \"Loss %s\" %(model.loss_function), 'loss')\n",
        "for metric in model.metric_train:\n",
        "  plot_train_history(h, 'Metric %s' %(metric.name), metric.name)\n",
        "\n",
        "prediction_vad, metrics_vad = model.predict_evaluate_custom(vad_dataset, \n",
        "                                                            label='vad', \n",
        "                                                            rescale=True)\n",
        "for label, m in metrics_vad:\n",
        "  print(label, m.numpy())\n",
        "\n",
        "for _ in np.arange(10):\n",
        "  plot_results(vad, prediction_vad, history, future)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkL3RXL1KzaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = copy.deepcopy(standard_inputs)\n",
        "inputs['teacher_forcing'] = True\n",
        "inputs['bernouilli_proba'] = 0.9\n",
        "inputs['use_attention'] = True\n",
        "inputs['attention_score'] = 'dot-product'\n",
        "inputs['dense_dim'] = None\n",
        "\n",
        "model = Seq2SeqModel(**inputs)\n",
        "h = model.fit_custom(train_dataset=train_dataset, epochs=epochs, vad_dataset=vad_dataset)\n",
        "\n",
        "plot_train_history(h, \"Loss %s\" %(model.loss_function), 'loss')\n",
        "for metric in model.metric_train:\n",
        "  plot_train_history(h, 'Metric %s' %(metric.name), metric.name)\n",
        "\n",
        "prediction_vad, metrics_vad = model.predict_evaluate_custom(vad_dataset, \n",
        "                                                            label='vad', \n",
        "                                                            rescale=True)\n",
        "for label, m in metrics_vad:\n",
        "  print(label, m.numpy())\n",
        "\n",
        "for _ in np.arange(10):\n",
        "  plot_results(vad, prediction_vad, history, future)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHPNF9utCA4k",
        "colab_type": "text"
      },
      "source": [
        "### Results with best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx7oeDVNCD2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = copy.deepcopy(standard_inputs)\n",
        "inputs['teacher_forcing'] = False\n",
        "inputs['use_attention'] = False\n",
        "inputs['dense_dim'] = None\n",
        "\n",
        "epochs = 60\n",
        "\n",
        "model = Seq2SeqModel(**inputs)\n",
        "h = model.fit_custom(train_dataset=train_dataset, epochs=epochs, vad_dataset=vad_dataset)\n",
        "prediction, metrics = model.predict_evaluate_custom(test_dataset, label='test', \n",
        "                                                    rescale=True)\n",
        "for _ in np.arange(10):\n",
        "  plot_results(test, prediction, history, future)\n",
        "\n",
        "mae = [v for l, v in metrics if 'mae' in l][0].numpy()\n",
        "mape = [v for l, v in metrics if 'mape' in l][0].numpy()\n",
        "\n",
        "print(\"Mean Average Error in test is %.4f mg/dl)\" % mae)\n",
        "print(\"Mean Average Percentage Error in test is %.4f %%\" % mape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXywFY9RTEny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}