{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "seq2seq_prediction_attention_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSmCiYhICymz",
        "colab_type": "text"
      },
      "source": [
        "# Sequence to Sequence models for sugar level prediction\n",
        "\n",
        "Seq2Seq modelling with Recurrent Neural Networks, \n",
        "take a look at https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
        "\n",
        "We also try teacher-forcing training, but it does not seem to provide good results in inference. \n",
        "\n",
        "Finally, we use ConvNets for creating input features to the encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Bf6QtFIDWP_",
        "colab_type": "code",
        "outputId": "32bc6d23-acc8-4aa6-acaf-6130297312b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNKe6wt8Cym-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import copy\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZTMhkmyDR0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Dense, LSTM, Input, RepeatVector, concatenate\n",
        "from tensorflow.keras.layers import Embedding, GaussianNoise, Reshape, Dropout \n",
        "from tensorflow.keras.layers import Convolution1D, AveragePooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import MAE\n",
        "from tensorflow.keras.metrics import MAPE, MSE\n",
        "from tensorflow.keras.backend import clear_session\n",
        "from tensorflow.keras.constraints import max_norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2vCNO9-PfN3",
        "colab_type": "text"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3KZB7NjEIl0",
        "colab_type": "text"
      },
      "source": [
        "### Load datasets\n",
        "\n",
        "Each dataset consits of sequences of `history`+`future` points, with 4 features: \n",
        "\n",
        "* time interval: days counted starting from the end of the `history` of the sequence. Thus, for points in the `history`, this feature takes negatuve values, while for points in the `future`, it's positive. \n",
        "* hour: hour of the day, divided by 24.\n",
        "* day of week: day of the week in numbers ('Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3, 'Friday': 4, 'Saturday': 5, 'Sunday': 6), divided by 7.\n",
        "* sugar level: recorded sugar level, scaled with min/max scaler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRgpiZSoEH2x",
        "colab_type": "code",
        "outputId": "d8e3feac-0ff4-48e3-dfba-effd23b27bb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vdrtT6jCynO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root = \"/content/drive/My Drive/Colab Notebooks/sugar_level_prediction/data/processed/normalized\"\n",
        "lastdate = '2020-01-16'\n",
        "root = os.path.join(root, lastdate)\n",
        "\n",
        "history = 3 * 24 * 4\n",
        "future = 3 * 4\n",
        "\n",
        "noise = 0.0\n",
        "replace = False\n",
        "\n",
        "train = np.load(os.path.join(root, \"noise_%s_train_steps_%s_replace_%s.npy\" %(noise, int(history), replace)))\n",
        "vad = np.load(os.path.join(root, \"noise_%s_vad_steps_%s_replace_%s.npy\" %(noise, int(history), replace)))\n",
        "test = np.load(os.path.join(root, \"noise_%s_test_steps_%s_replace_%s.npy\" %(noise, int(history), replace)))\n",
        "\n",
        "assert history+future == train.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_mOC2M-qeTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1203)\n",
        "np.random.shuffle(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGcDkv-LJWUw",
        "colab_type": "text"
      },
      "source": [
        "###  Split data \n",
        "\n",
        "Each input sequence has both the features and labels (x and y, if you wish), so we have to separate them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk_8mzP9JrcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_features_labels(data, history, future, start_char):\n",
        "    \"\"\"\n",
        "    Method to separate historic and future events (features and labels). \n",
        "    It returns input data for the encoder and decoder, and the output data \n",
        "    for the decoder. The input data for the decoder is just the output data \n",
        "    of the decoder, shifted by one step. \n",
        "\n",
        "    :param data: numpy ndarray with sequences of history+future points, and 4 attributes \n",
        "      (time_interval, hour_of_day, day_of_week, patient_id, sugar_level). It has shape (?, history+future, 5)\n",
        "    :param history: number of points for the features\n",
        "    :param future: number of points for the labels\n",
        "    :param start_char: start charcter for input sequences to the decoder\n",
        "    :return three numpy arrays with the input data for the encoder (shape=(?, history, 5))\n",
        "        and decoder (shape=(?, future+1, 1)), and the output data for the decoder\n",
        "        (shape=(?, future+1, 1))\n",
        "    \"\"\"\n",
        "    # split features and labels . Note that for the later, we only keep the \n",
        "    # feature with the sugar level, which constitutes our target\n",
        "    yf, yl = data[:, :history], data[:, history:history+future, -1]\n",
        "    \n",
        "    # add start of sentence to labels (input to the decoder)\n",
        "    yl_input = start_char * np.ones(shape=(yl.shape[0], yl.shape[1]+1))\n",
        "    yl_input[:, 1:] = yl\n",
        "    # add end of sentence to labels (output of the decoder)\n",
        "    yl_output = start_char * np.ones(shape=(yl.shape[0], yl.shape[1]+1))\n",
        "    yl_output[:, :-1] = yl\n",
        "    \n",
        "    # add new dimension at the end of input/output arrays to the decoder\n",
        "    yl_input = yl_input[:, :, np.newaxis].astype(np.float32)\n",
        "    yl_output = yl_output[:, :, np.newaxis].astype(np.float32)\n",
        "    \n",
        "    return (yf, yl_input), yl_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE0-L8XQObFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_char = -1.0\n",
        "train_data = split_features_labels(train, history, future, start_char)\n",
        "vad_data = split_features_labels(vad, history, future, start_char)\n",
        "test_data = split_features_labels(test, history, future, start_char)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26Ib_uLdCyns",
        "colab_type": "code",
        "outputId": "f2dcb08b-aeba-4dd5-85f5-1b928f8a3e1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "train_data[0][0].shape, train_data[0][1].shape, train_data[1].shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((144942, 288, 5), (144942, 13, 1), (144942, 13, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiizJISGiUyV",
        "colab_type": "text"
      },
      "source": [
        "## Some auxiliar functions \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3vlqUipiiLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_train_history(history, title, metric):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    loss = history[metric]\n",
        "    val_loss = history['val_%s' %metric]\n",
        "\n",
        "    epochs = range(len(loss))\n",
        "\n",
        "    plt.plot(epochs, loss, 'b', label='Training %s' %metric)\n",
        "    plt.plot(epochs, val_loss, 'r', label='Validation %s' %metric)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.yscale('log')\n",
        "    plt.gca().grid(axis='y', which='minor')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_results(dataset, prediction, history, future):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # get random idx\n",
        "    idx = np.random.randint(0, dataset.shape[0])\n",
        "\n",
        "    # get intervals\n",
        "    xf = dataset[idx, :history, 0]\n",
        "    xl = dataset[idx, history:history+future, 0]\n",
        "    \n",
        "    # get sequences\n",
        "    input_seq_ = dataset[idx, :history, -1]\n",
        "    target_seq_ = dataset[idx, history:history+future, -1]\n",
        "    decoded_seq_ = prediction[idx, :]\n",
        "\n",
        "    ymin = input_seq_.min()-0.1\n",
        "    ymax = input_seq_.max()+0.1\n",
        "    \n",
        "    plt.plot(xf, input_seq_, '-b', markersize=2, linewidth=2, label='historic data')\n",
        "    plt.plot(xl, target_seq_, '.--b', markersize=2, linewidth=2, label='future data')\n",
        "    plt.plot(xl, decoded_seq_, '.--k', markersize=2, linewidth=2, label='prediction')\n",
        "    plt.vlines(x=0, ymin=ymin, ymax=ymax, colors='k', linewidth=1, linestyles='--')\n",
        "\n",
        "    plt.ylim(ymin, ymax)\n",
        "    plt.xlim(-0.3-history/4/24, future/4/24+0.3)\n",
        "    \n",
        "    plt.ylabel('Glucose evolution (arb.units)')\n",
        "    plt.xlabel('Days before/after start prediction')\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlM_vedWxNDR",
        "colab_type": "text"
      },
      "source": [
        "## Seq2Seq model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACLITmI9xSp4",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jma2DOYxLVFg",
        "colab_type": "text"
      },
      "source": [
        "#### Class ProcessInput "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rYj5zjFxlS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ProcessInput(tf.keras.Model):\n",
        "  def __init__(self, dense_dim, dense_activation, gaussian_noise, num_ids,\n",
        "               day_of_week_dim, hour_of_day_dim, patient_id_dim,\n",
        "               **kwargs):\n",
        "    super(ProcessInput, self).__init__(**kwargs)\n",
        "\n",
        "    # define attribute variables\n",
        "    self.dense_dim = dense_dim\n",
        "    self.dense_activation = dense_activation\n",
        "    self.gaussian_noise = gaussian_noise\n",
        "    self.num_ids = num_ids\n",
        "    self.day_of_week_dim = day_of_week_dim \n",
        "    self.hour_of_day_dim = hour_of_day_dim \n",
        "    self.patient_id_dim = patient_id_dim\n",
        "\n",
        "    # Define embedding layers for Input categorical layers\n",
        "    self.day_of_week_emb = Embedding(input_dim=7, output_dim=self.day_of_week_dim)\n",
        "    self.hour_of_day_emb = Embedding(input_dim=24, output_dim=self.hour_of_day_dim)\n",
        "    self.patient_id_emb = Embedding(input_dim=self.num_ids, output_dim=self.patient_id_dim)\n",
        "\n",
        "    # gaussian noise layer \n",
        "    if self.gaussian_noise: \n",
        "      self.gn_layer = GaussianNoise(stddev=self.gaussian_noise)\n",
        "    else: \n",
        "      self.gn_layer = None\n",
        "    # dense layer\n",
        "    if self.dense_dim: \n",
        "      # TODO use TimeDistributed before the Dense? I believe the layer is broadcasted here\n",
        "      self.dense = Dense(self.dense_dim, activation=self.dense_activation)\n",
        "    else:\n",
        "        self.dense = None\n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    # split inputs\n",
        "    time_interval, hour_of_day, day_of_week, patient_id, sugar_level = inputs\n",
        "    # create embeddings for categorical variables\n",
        "    day_of_week = self.day_of_week_emb(day_of_week)\n",
        "    hour_of_day = self.hour_of_day_emb(hour_of_day)\n",
        "    patient_id = self.patient_id_emb(patient_id)\n",
        "\n",
        "    # add noise\n",
        "    if self.gn_layer:\n",
        "        sugar_level = self.gn_layer(sugar_level, training=training)\n",
        "    \n",
        "    # Concatenate all inputs toghether, and pass them through a dense layer\n",
        "    output = concatenate([time_interval, hour_of_day, day_of_week, \n",
        "                          patient_id, sugar_level], axis=-1)\n",
        "    if self.dense:\n",
        "        output = self.dense(output)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RETckryiLdQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# simple testing\n",
        "test_layer = ProcessInput(dense_dim=None, dense_activation=None, gaussian_noise=0.1, \n",
        "                          num_ids=7, day_of_week_dim=10, hour_of_day_dim=10, \n",
        "                          patient_id_dim=10)\n",
        "\n",
        "assert test_layer.dense == None\n",
        "assert test_layer.gaussian_noise == 0.1\n",
        "assert test_layer.gn_layer.stddev == 0.1\n",
        "\n",
        "out = test_layer([Input(shape=(None, 1)), Input(shape=(None,)), Input(shape=(None,)), \n",
        "                  Input(shape=(None,)), Input(shape=(None, 1))])\n",
        "assert out.get_shape().as_list() == [None, None, 32]\n",
        "\n",
        "out = test_layer([Input(shape=(16, 1)), Input(shape=(16,)), Input(shape=(16,)), \n",
        "                  Input(shape=(16,)), Input(shape=(16, 1))])\n",
        "assert out.get_shape().as_list() == [None, 16, 32]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7IGhN8jNFrB",
        "colab_type": "text"
      },
      "source": [
        "#### Class Encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6mNQeONNjSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, stacked_units, kernel_mn, recurrent_mn, dropout, **kwargs):\n",
        "    super(Encoder, self).__init__(**kwargs)\n",
        "    self.stacked_units = stacked_units\n",
        "    self.kernel_mn = kernel_mn\n",
        "    self.recurrent_mn = recurrent_mn\n",
        "    self.dropout = dropout\n",
        "\n",
        "    # for loop to stack lstms, with returning sequences\n",
        "    self.lstms = self.stack_lstms()\n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    for lstm in self.lstms[:-1]:\n",
        "      inputs = lstm(inputs, training=training)\n",
        "    encoder_outputs, state_h, state_c = self.lstms[-1](inputs ,training=training)\n",
        "    encoder_states = [state_h, state_c]  \n",
        "    return encoder_outputs, encoder_states\n",
        "\n",
        "  def stack_lstms(self):\n",
        "    # for loop to stack lstms, with returning sequences\n",
        "    lstms = []\n",
        "    for units in self.stacked_units[:-1]:\n",
        "      lstm_ = LSTM(\n",
        "        units, \n",
        "        return_sequences=True, \n",
        "        kernel_constraint=max_norm(self.kernel_mn),\n",
        "        recurrent_constraint=max_norm(self.recurrent_mn), \n",
        "        dropout=self.dropout)\n",
        "      lstms.append(lstm_)\n",
        "\n",
        "    # the last lstm returns the states as well as the sequences\n",
        "    lstm_ = LSTM(\n",
        "      self.stacked_units[-1], \n",
        "      return_state=True, \n",
        "      return_sequences=True, \n",
        "      kernel_constraint=max_norm(self.kernel_mn),\n",
        "      recurrent_constraint=max_norm(self.recurrent_mn), \n",
        "      dropout=self.dropout)\n",
        "    lstms.append(lstm_)\n",
        "    return lstms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgi6D18dVW5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# simple testing\n",
        "input_test_layer = ProcessInput(\n",
        "  dense_dim=None, dense_activation=None, gaussian_noise=0.0, num_ids=7, \n",
        "  day_of_week_dim=10, hour_of_day_dim=10, patient_id_dim=10)\n",
        "\n",
        "input_test_layer = input_test_layer(\n",
        "  [Input(shape=(16, 1)), Input(shape=(16,)), Input(shape=(16,)), \n",
        "   Input(shape=(16,)), Input(shape=(16, 1))])\n",
        "\n",
        "test_enc = Encoder(stacked_units=[5, 5], kernel_mn=0.3, recurrent_mn=0.1, dropout=0.3)\n",
        "enc_lstms = test_enc.lstms\n",
        "assert len(enc_lstms) == 2\n",
        "\n",
        "encoder_outputs, encoder_states = test_enc(input_test_layer)\n",
        "assert len(encoder_states) == 2\n",
        "assert encoder_outputs.get_shape().as_list() == [None, 16, 5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOTtCAOvfUVm",
        "colab_type": "text"
      },
      "source": [
        "#### Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnhhI_pkfizQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, kernel_mn):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.units = units\n",
        "    self.kernel_mn = kernel_mn\n",
        "    \n",
        "    # Weights relating to target states (decoder typically)\n",
        "    self.source_weight = tf.keras.layers.Dense(units, kernel_constraint=max_norm(kernel_mn))  \n",
        "    # Weights relating to source states (encoder typically)\n",
        "    self.target_weight = tf.keras.layers.Dense(units, kernel_constraint=max_norm(kernel_mn))  \n",
        "    # score transformation\n",
        "    self.score_weight = tf.keras.layers.Dense(1)  \n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    # target is also referred as query, and source as values.\n",
        "    target, source = inputs\n",
        "    # target must be a hidden vector of shape=(None, hidden_size), \n",
        "    # while source has shape=(None, steps, hidden_size)\n",
        "\n",
        "    # expand the dimension of the target, (None, 1, hidden_size)\n",
        "    target_with_time_axis = tf.expand_dims(target, axis=1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.score_weight(\n",
        "        tf.nn.tanh(\n",
        "            self.source_weight(source) + self.target_weight(target_with_time_axis)\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * source\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M3uRLsT3oBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# simple testing\n",
        "input_test_layer = ProcessInput(\n",
        "  dense_dim=None, dense_activation=None, gaussian_noise=0.0, num_ids=7, \n",
        "  day_of_week_dim=10, hour_of_day_dim=10, patient_id_dim=10)\n",
        "\n",
        "input_test_layer = input_test_layer(\n",
        "  [Input(shape=(16, 1)), Input(shape=(16,)), Input(shape=(16,)), \n",
        "   Input(shape=(16,)), Input(shape=(16, 1))])\n",
        "\n",
        "test_enc = Encoder(stacked_units=[5, 5], kernel_mn=0.3, recurrent_mn=0.1, dropout=0.3)\n",
        "encoder_outputs, encoder_states = test_enc(input_test_layer)\n",
        "encoder_hidden = encoder_states[0]\n",
        "\n",
        "attention_layer = BahdanauAttention(10, kernel_mn=0.2)\n",
        "attention_result, attention_weights = attention_layer([encoder_hidden, encoder_outputs])\n",
        "\n",
        "assert attention_layer.source_weight.get_input_shape_at(0) == (None, 16, 5)\n",
        "assert attention_layer.source_weight.get_output_shape_at(0) == (None, 16, 10)\n",
        "\n",
        "# why this does not work?\n",
        "# attention_layer.score_weight.get_output_shape_at(0)\n",
        "\n",
        "assert attention_result.get_shape()[1] == 5\n",
        "assert attention_weights.get_shape().as_list() == [None, 16, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAq_Rd-FfenX",
        "colab_type": "text"
      },
      "source": [
        "#### Class decoder\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu6pRZ0dfjXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(Encoder):\n",
        "  def __init__(self, stacked_units, kernel_mn, recurrent_mn, dropout, \n",
        "               mlp_units, activation_mlp, mlp_mn, attention_kernel_mn,\n",
        "               **kwargs):\n",
        "    super(Decoder, self).__init__(stacked_units, kernel_mn, recurrent_mn, dropout, **kwargs)\n",
        "    self.mlp_units = mlp_units \n",
        "    self.activation_mlp = activation_mlp\n",
        "    self.mlp_mn = mlp_mn\n",
        "    self.attention_kernel_mn = attention_kernel_mn\n",
        "\n",
        "    self.mlp = self.mlp()\n",
        "    self.attention = BahdanauAttention(units=self.stacked_units[-1], \n",
        "                                       kernel_mn=attention_kernel_mn)\n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    # split inputs in teacher forcing input (can be None), target state (decoder at t-1)\n",
        "    # and source states (outputs from the encoder)\n",
        "    teacher_input, target, source_seq = inputs\n",
        "    context_vector, attention_weights = self.attention(inputs=[target, source_seq], \n",
        "                                                       training=training)\n",
        "    \n",
        "    # concatenate context vector and tf_input\n",
        "    context_vector = tf.expand_dims(context_vector, 1)  # shape=(None, 1, dec_hidden_size)\n",
        "    if teacher_input is not None:\n",
        "      merged = tf.concat([context_vector, teacher_input], axis=-1)\n",
        "    else:\n",
        "      merged = context_vector\n",
        "    \n",
        "    # pass the merged vector through the stack of LSTMs\n",
        "    for lstm in self.lstms[:-1]:\n",
        "      merged = lstm(merged, training=training)\n",
        "    decoder_outputs, state_h, state_c = self.lstms[-1](merged, training=training)\n",
        "    decoder_states = [state_h, state_c]  \n",
        "    \n",
        "    # reshape output, as we pass one step at a time\n",
        "    decoder_outputs = tf.reshape(decoder_outputs, (-1, decoder_outputs.shape[2]))\n",
        "    \n",
        "    # pass the output to the mlp. output shape=(None, 1)\n",
        "    out = decoder_outputs\n",
        "    for dense in self.mlp:\n",
        "      out = dense(out)\n",
        "    return out, decoder_states, attention_weights\n",
        "\n",
        "  def mlp(self):\n",
        "    # MLP for output sequences\n",
        "    mlp_stack = []\n",
        "    for neurons in self.mlp_units:\n",
        "        decoder_dense = Dense(neurons, activation=self.activation_mlp, \n",
        "                              kernel_constraint=max_norm(self.mlp_mn))\n",
        "        mlp_stack.append(decoder_dense)\n",
        "    return mlp_stack"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzBo7FSFdGB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# simple testing\n",
        "input_test_layer = ProcessInput(\n",
        "  dense_dim=None, dense_activation=None, gaussian_noise=0.0, num_ids=7, \n",
        "  day_of_week_dim=10, hour_of_day_dim=10, patient_id_dim=10)\n",
        "\n",
        "input_test_layer = input_test_layer(\n",
        "  [Input(shape=(16, 1)), Input(shape=(16,)), Input(shape=(16,)), \n",
        "   Input(shape=(16,)), Input(shape=(16, 1))])\n",
        "\n",
        "test_enc = Encoder(stacked_units=[5, 5], kernel_mn=0.3, recurrent_mn=0.1, dropout=0.3)\n",
        "encoder_outputs, encoder_states = test_enc(input_test_layer)\n",
        "encoder_hidden = encoder_states[0]\n",
        "\n",
        "# test with teacher-forcing\n",
        "test_dec = Decoder(stacked_units=[5, 5], kernel_mn=0.3, recurrent_mn=0.1, \n",
        "                   dropout=0.3, mlp_units=[4, 1], activation_mlp='relu', \n",
        "                   mlp_mn=0.4, attention_kernel_mn=0.2)\n",
        "input_teacher = Input(shape=(1, 1))\n",
        "\n",
        "# initial step\n",
        "input_dec = [input_teacher, encoder_hidden, encoder_outputs]\n",
        "decoder_outputs, decoder_states, attention_weights = test_dec(input_dec)\n",
        "\n",
        "assert len(decoder_states) == 2\n",
        "assert decoder_states[0].get_shape().as_list() == [None, 5]\n",
        "assert decoder_outputs.get_shape().as_list() == [None, 1]\n",
        "assert attention_weights.get_shape().as_list() == [None, 16, 1]\n",
        "\n",
        "# subsequent steps. \n",
        "decoder_hidden = decoder_states[0]\n",
        "input_dec = [input_teacher, decoder_hidden, encoder_outputs]\n",
        "decoder_outputs, decoder_states, attention_weights = test_dec(input_dec)\n",
        "\n",
        "assert decoder_outputs.get_shape().as_list() == [None, 1]\n",
        "assert attention_weights.get_shape().as_list() == [None, 16, 1]\n",
        "\n",
        "# Finally test without teacher-forcing\n",
        "test_dec = Decoder(stacked_units=[5, 5], kernel_mn=0.3, recurrent_mn=0.1, \n",
        "                   dropout=0.3, mlp_units=[4, 1], activation_mlp='relu', \n",
        "                   mlp_mn=0.4, attention_kernel_mn=0.2)\n",
        "input_teacher = None\n",
        "\n",
        "# initial step\n",
        "input_dec = [input_teacher, encoder_hidden, encoder_outputs]\n",
        "decoder_outputs, decoder_states, attention_weights = test_dec(input_dec)\n",
        "\n",
        "assert len(decoder_states) == 2\n",
        "assert decoder_states[0].get_shape().as_list() == [None, 5]\n",
        "assert decoder_outputs.get_shape().as_list() == [None, 1]\n",
        "assert attention_weights.get_shape().as_list() == [None, 16, 1]\n",
        "\n",
        "# subsequent steps. \n",
        "decoder_hidden = decoder_states[0]\n",
        "input_dec = [input_teacher, decoder_hidden, encoder_outputs]\n",
        "decoder_outputs, decoder_states, attention_weights = test_dec(input_dec)\n",
        "assert decoder_outputs.get_shape().as_list() == [None, 1]\n",
        "assert attention_weights.get_shape().as_list() == [None, 16, 1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q7AaPcRuNgM",
        "colab_type": "text"
      },
      "source": [
        "#### Complete model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX5IgZ3vjd_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2SeqModel(tf.keras.Model):\n",
        "  def __init__(self, dense_dim, dense_activation, gaussian_noise,\n",
        "               num_ids, day_of_week_dim, hour_of_day_dim, patient_id_dim,\n",
        "               enc_stacked_units, enc_kernel_mn, enc_recurrent_mn,\n",
        "               enc_dropout, dec_stacked_units, dec_kernel_mn, dec_recurrent_mn, \n",
        "               dec_dropout, attention_kernel_mn, mlp_units, activation_mlp, \n",
        "               mlp_mn, teacher_forcing, start_char, learning_rate, clipvalue, \n",
        "               **kwargs):\n",
        "    super(Seq2SeqModel, self).__init__(**kwargs)\n",
        "\n",
        "    # clear worksapce\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    # ProcessInput parameters\n",
        "    self.dense_dim = dense_dim \n",
        "    self.dense_activation = dense_activation \n",
        "    self.gaussian_noise = gaussian_noise\n",
        "    self.num_ids = num_ids \n",
        "    self.day_of_week_dim = day_of_week_dim \n",
        "    self.hour_of_day_dim = hour_of_day_dim \n",
        "    self.patient_id_dim = patient_id_dim\n",
        "\n",
        "    # Encoder Parameters\n",
        "    self.enc_stacked_units = enc_stacked_units \n",
        "    self.enc_kernel_mn = enc_kernel_mn \n",
        "    self.enc_recurrent_mn = enc_recurrent_mn\n",
        "    self.enc_dropout = enc_dropout \n",
        "\n",
        "    # Decoder parameters\n",
        "    self.dec_stacked_units = dec_stacked_units \n",
        "    self.dec_kernel_mn = dec_kernel_mn \n",
        "    self.dec_recurrent_mn = dec_recurrent_mn \n",
        "    self.dec_dropout = dec_dropout \n",
        "    self.mlp_units = mlp_units \n",
        "    self.activation_mlp = activation_mlp \n",
        "    self.mlp_mn = mlp_mn \n",
        "    self.attention_kernel_mn = attention_kernel_mn\n",
        "\n",
        "    # Optimization parameters\n",
        "    self.learning_rate = learning_rate \n",
        "    self.clipvalue = clipvalue\n",
        "\n",
        "    # Others\n",
        "    self.teacher_forcing = teacher_forcing\n",
        "    self.start_char = start_char\n",
        "\n",
        "    # build and compile\n",
        "    self.build()\n",
        "    self.compile()\n",
        "\n",
        "  def build(self):\n",
        "    self.process_layer = ProcessInput(\n",
        "      dense_dim=self.dense_dim, dense_activation=self.dense_activation, \n",
        "      gaussian_noise=self.gaussian_noise, num_ids=self.num_ids, \n",
        "      day_of_week_dim=self.day_of_week_dim, hour_of_day_dim=self.hour_of_day_dim, \n",
        "      patient_id_dim=self.patient_id_dim)\n",
        "\n",
        "    self.encoder = Encoder(\n",
        "      stacked_units=self.enc_stacked_units, kernel_mn=self.enc_kernel_mn, \n",
        "      recurrent_mn=self.enc_recurrent_mn, dropout=self.enc_dropout)\n",
        "\n",
        "    self.decoder = Decoder(\n",
        "      stacked_units=self.dec_stacked_units, kernel_mn=self.dec_kernel_mn, \n",
        "      recurrent_mn=self.dec_recurrent_mn, dropout=self.dec_dropout, \n",
        "      mlp_units=self.mlp_units, activation_mlp=self.activation_mlp, \n",
        "      mlp_mn=self.mlp_mn, attention_kernel_mn=self.attention_kernel_mn)\n",
        "\n",
        "  def compile(self):\n",
        "    self.optimizer = tf.keras.optimizers.Adam(\n",
        "      learning_rate=self.learning_rate, clipvalue=self.clipvalue)\n",
        "    self.loss_function = tf.keras.losses.MeanSquaredError()\n",
        "    self.metric_functions = [tf.keras.metrics.MeanAbsoluteError()]\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(self, inp, tar):\n",
        "    loss = 0\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "      input_enc = self.process_layer(inp, training=True)\n",
        "      enc_outputs, enc_states = self.encoder(input_enc, training=True)\n",
        "      dec_hidden = enc_states[0]\n",
        "      \n",
        "      # provide start_char as first input char\n",
        "      dec_input = self.start_char * tf.ones_like(tar[:, 0, :])\n",
        "      \n",
        "      for t in range(0, tar.shape[1]-1):\n",
        "        # expand dim of dec_input\n",
        "        dec_input = tf.expand_dims(dec_input, axis=1)\n",
        "        # passing enc_output to the decoder\n",
        "        predictions, dec_states, _ = self.decoder(inputs=[dec_input, dec_hidden, enc_outputs], \n",
        "                                                  training=True)\n",
        "        dec_hidden = dec_states[0]\n",
        "\n",
        "        loss += self.loss_function(tar[:, t, :], predictions)\n",
        "        _ = [m.update_state(tar[:, t, :], predictions) for m in self.metric_functions]\n",
        "          \n",
        "        \n",
        "        # teacher-forcing\n",
        "        if self.teacher_forcing is not None:\n",
        "          dec_input = tar[:, t, :]\n",
        "        else:\n",
        "          dec_input = predictions\n",
        "    \n",
        "    batch_loss = loss / int(tar.shape[1])\n",
        "    variables = self.process_layer.trainable_variables + \\\n",
        "      self.encoder.trainable_variables + \\\n",
        "      self.decoder.trainable_variables \n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss\n",
        "\n",
        "  @tf.function\n",
        "  def inference_step(self, inp, steps, batch_size):\n",
        "\n",
        "    # initialize output tensor\n",
        "    prediction = []\n",
        "\n",
        "    # forward pass through process_layer and encoder\n",
        "    input_enc = self.process_layer(inp, training=False)\n",
        "    enc_outputs, enc_states = self.encoder(input_enc, training=False)\n",
        "    dec_hidden = enc_states[0]\n",
        "    \n",
        "    # provide start_char as first input char\n",
        "    dec_input = self.start_char * tf.ones(shape=(batch_size, 1))\n",
        "    \n",
        "    # forward pass through the decoder\n",
        "    for t in range(0, steps):\n",
        "      # expand dim of dec_input\n",
        "      dec_input = tf.expand_dims(dec_input, axis=1)\n",
        "      # passing enc_output to the decoder\n",
        "      step_prediction, dec_states, _ = self.decoder([dec_input, dec_hidden, enc_outputs])\n",
        "      dec_hidden = dec_states[0]\n",
        "      dec_input = step_prediction\n",
        "      prediction.append(tf.expand_dims(dec_input, axis=1))\n",
        "    \n",
        "    # concat predictions\n",
        "    prediction = tf.concat(prediction, axis=1)\n",
        "    \n",
        "    return prediction\n",
        "\n",
        "  def fit_custom(self, train_dataset, epochs, vad_dataset):\n",
        "    history = {}\n",
        "    history['loss'] = []\n",
        "    history['val_loss'] = []\n",
        "    for m in self.metric_functions:\n",
        "      history[m.name] = []  \n",
        "      history['val_%s' % (m.name)] = []  \n",
        "\n",
        "    # steps per epoch in train and validation\n",
        "    len_train = tf.data.experimental.cardinality(train_dataset).numpy()\n",
        "    len_vad = tf.data.experimental.cardinality(vad_dataset).numpy()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "    \n",
        "      start = time.time()\n",
        "      print('Epoch ({}/{})'.format(epoch+1, epochs))\n",
        "      \n",
        "      pbar = tf.keras.utils.Progbar(len_train + len_vad)\n",
        "\n",
        "      # training loop\n",
        "      loop = enumerate(train_dataset.take(len_train))\n",
        "\n",
        "      total_loss = 0\n",
        "\n",
        "      for (batch, (inp, targ)) in loop:\n",
        "        batch_loss = self.train_step(inp, targ)\n",
        "        total_loss += batch_loss\n",
        "        total_metric = [(m.name, m.result()) for m in self.metric_functions]\n",
        "        pbar.update(batch, values=[(\"loss\", batch_loss)] + total_metric)\n",
        "      \n",
        "      total_loss  /= float(len_train)\n",
        "      history['loss'].append(total_loss.numpy())\n",
        "      for m in self.metric_functions:\n",
        "        history[m.name].append(m.result().numpy())\n",
        "        m.reset_states()\n",
        "\n",
        "      # validation loop (inference and eval)\n",
        "      loop = enumerate(vad_dataset)\n",
        "      \n",
        "      # number of steps (we do not perform inference for the EOS)\n",
        "      batch_size, steps = [(tar.shape[0], tar.shape[1]-1) for inp, tar in vad_dataset.take(1)][0]\n",
        "      \n",
        "      total_loss = 0\n",
        "      \n",
        "      for (batch, (inp, targ)) in loop:\n",
        "        prediction = self.inference_step(inp, steps, batch_size)\n",
        "        batch_loss = self.loss_function(prediction, targ[:, :-1, :])\n",
        "        total_loss += batch_loss\n",
        "        [m.update_state(targ[:, :-1, :], prediction) for m in self.metric_functions]\n",
        "        total_metric = [('val_%s' %(m.name), m.result()) for m in self.metric_functions]\n",
        "        pbar.update(batch, values=[(\"val_loss\", batch_loss)] + total_metric)  \n",
        "      \n",
        "      total_loss  /= float(len_vad)\n",
        "      history['val_loss'].append(total_loss.numpy())\n",
        "      for m in self.metric_functions:\n",
        "        history['val_%s' %(m.name)].append(m.result().numpy())\n",
        "        m.reset_states()\n",
        "\n",
        "      print(' ({:.0f} sec)\\n'.format( time.time() - start))\n",
        "    return history\n",
        "\n",
        "  def predict_custom(self, dataset):\n",
        "    start = time.time()\n",
        "\n",
        "    # inference loop\n",
        "    loop = enumerate(dataset)\n",
        "\n",
        "    # number of steps (we do not perform inference for the EOS)\n",
        "    batch_size, steps = [(tar.shape[0], tar.shape[1]-1) for inp, tar in dataset.take(1)][0]\n",
        "\n",
        "    # num steps and progress bar    \n",
        "    len_ = tf.data.experimental.cardinality(dataset).numpy()\n",
        "    pbar = tf.keras.utils.Progbar(len_)\n",
        "\n",
        "    prediction = []\n",
        "    for (batch, (inp, targ)) in loop:\n",
        "      # prediction has shape=(batch_size, steps, 1)\n",
        "      prediction.append(self.inference_step(inp, steps, batch_size))\n",
        "      pbar.update(batch)\n",
        "    prediction = tf.concat(prediction, axis=0).numpy()\n",
        "    print(' ({:.0f} sec)\\n'.format( time.time() - start))\n",
        "    return prediction\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEFXP_uZ7uv5",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7ktp8XoLq0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_columns(data):\n",
        "    \"\"\"\n",
        "    Split and prepare input data\n",
        "\n",
        "    :param data: array with four columns: time_interval, hour_of_day, \n",
        "        day_of_week, sugar_level\n",
        "    \"\"\"\n",
        "    time_interval, hour_of_day, day_of_week, patient_id, sugar_level = np.split(data, \n",
        "        indices_or_sections=5, axis=2)\n",
        "    # squeeze categorical features (ncessary for the Embedding layer)\n",
        "    hour_of_day = np.squeeze(hour_of_day, axis=2)\n",
        "    day_of_week = np.squeeze(day_of_week, axis=2)\n",
        "    patient_id = np.squeeze(patient_id, axis=2)\n",
        "    return (time_interval, hour_of_day, day_of_week, patient_id, sugar_level)\n",
        "\n",
        "def get_dataset(data, batch_size, shuffle=False, drop_remainder=False):\n",
        "  input_ = get_columns(data[0][0])\n",
        "  target_ = data[1]\n",
        "\n",
        "  buffer_size = input_[0].shape[0]\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((input_, target_))\n",
        "  if shuffle:\n",
        "    dataset = dataset.shuffle(buffer_size)\n",
        "  dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRCUDHegdANq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 300\n",
        "steps_per_epoch = train_data[1].shape[0] // batch_size\n",
        "train_dataset = get_dataset(train_data, batch_size, shuffle=True, drop_remainder=True) \n",
        "vad_dataset = get_dataset(vad_data, batch_size, shuffle=False, drop_remainder=True) \n",
        "test_dataset = get_dataset(test_data, batch_size, shuffle=False, drop_remainder=True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZF3oU66i4M6",
        "colab_type": "code",
        "outputId": "8b45fa31-c3e8-4ed1-9bb5-cd307c268594",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "num_identifiers = np.unique(train_data[0][0][:, :, 3]).size\n",
        "print(\"There are %s individuals\" % num_identifiers)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 7 individuals\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qZ0rWRdmwXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "standard_inputs = dict(\n",
        "  # ProcessInput params\n",
        "  dense_dim=80,\n",
        "  dense_activation='selu',\n",
        "  gaussian_noise=0.0,\n",
        "  num_ids=num_identifiers,\n",
        "  day_of_week_dim=20,\n",
        "  hour_of_day_dim=20,\n",
        "  patient_id_dim=20,\n",
        "  # Encoder params\n",
        "  enc_stacked_units=[128],\n",
        "  enc_kernel_mn=0.1,\n",
        "  enc_recurrent_mn=0.1,\n",
        "  enc_dropout=0.0,\n",
        "  # Decoder params\n",
        "  dec_stacked_units=[128], \n",
        "  dec_kernel_mn=0.1,\n",
        "  dec_recurrent_mn=0.1,\n",
        "  dec_dropout=0.0,\n",
        "  mlp_units=[64, 32, 8, 1],\n",
        "  activation_mlp='selu',\n",
        "  mlp_mn=0.3,\n",
        "  attention_kernel_mn=0.1, \n",
        "  teacher_forcing = None,\n",
        "  start_char = -1.0,\n",
        "  # Optimization params\n",
        "  learning_rate = 5e-4,\n",
        "  clipvalue = 0.5\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNApZ87ftxPf",
        "colab_type": "code",
        "outputId": "20e65254-28a0-4de7-8729-000d8f23d92b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "inputs = copy.deepcopy(standard_inputs)\n",
        "inputs['teacher_forcing'] = False\n",
        "inputs['enc_stacked_units'] = [64, 64]\n",
        "inputs['dec_stacked_units'] = [64, 64]\n",
        "inputs['dense_dim'] = None\n",
        "inputs['mlp_units'] = [32, 8, 1]\n",
        "# inputs['enc_dropout'] = 0.1\n",
        "inputs['dec_dropout'] = 0.1\n",
        "inputs['attention_kernel_mn'] = 1.0\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "model = Seq2SeqModel(**inputs)\n",
        "h = model.fit_custom(train_dataset=train_dataset, epochs=epochs, vad_dataset=vad_dataset)\n",
        "\n",
        "plot_train_history(h, \"Loss %s\" %(model.loss_function), 'loss')\n",
        "for metric in model.metric_functions:\n",
        "  plot_train_history(h, 'Metric %s' %(metric.name), metric.name)\n",
        "\n",
        "prediction_vad = model.predict_custom(vad_dataset)\n",
        "for _ in np.arange(10):\n",
        "  plot_results(vad, prediction_vad, history, future)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch (1/50)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}