{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "seq2seq_prediction_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSmCiYhICymz",
        "colab_type": "text"
      },
      "source": [
        "# Sequence to Sequence models for sugar level prediction\n",
        "\n",
        "For the first part, which make use of Recurrent Neural Networks, \n",
        "take a look at https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
        "\n",
        "The second part (to be done) will use the more advanced Transformer arquitecture. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Bf6QtFIDWP_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bbe57d43-3a00-45ec-dc50-41d08e4fcb20"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNKe6wt8Cym-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZTMhkmyDR0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Dense, LSTM, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import MAE\n",
        "from tensorflow.keras.metrics import MAPE\n",
        "from tensorflow.keras.backend import clear_session"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2vCNO9-PfN3",
        "colab_type": "text"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3KZB7NjEIl0",
        "colab_type": "text"
      },
      "source": [
        "### Load datasets\n",
        "\n",
        "Each dataset consits of sequences of `history`+`future` points, with 4 features: \n",
        "\n",
        "* time interval: days counted starting from the end of the `history` of the sequence. Thus, for points in the `history`, this feature takes negatuve values, while for points in the `future`, it's positive. \n",
        "* hour: hour of the day, divided by 24.\n",
        "* day of week: day of the week in numbers ('Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3, 'Friday': 4, 'Saturday': 5, 'Sunday': 6), divided by 7.\n",
        "* sugar level: recorded sugar level, scaled with min/max scaler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRgpiZSoEH2x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "a5e43a8a-26ac-4a29-97aa-74dbffe5f8b0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRcQBJPOESsA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7e881a71-cd56-43db-e3ee-eb78d1199000"
      },
      "source": [
        "!ls '/content/drive/My Drive/Colab Notebooks/sugar_level_prediction/data/'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "javi_measurements.csv  noisy0.0_test.npy  noisy0.0_train.npy  noisy0.0_vad.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vdrtT6jCynO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root = \"/content/drive/My Drive/Colab Notebooks/sugar_level_prediction/data/\"\n",
        "noise=0.0\n",
        "history = 100 \n",
        "future = 12\n",
        "\n",
        "train = np.load(os.path.join(root, \"noisy%s_train.npy\" %noise))\n",
        "vad = np.load(os.path.join(root, \"noisy%s_vad.npy\" %noise))\n",
        "test = np.load(os.path.join(root, \"noisy%s_test.npy\" %noise))\n",
        "\n",
        "train_steps = train.shape[0] // train_batch\n",
        "vad_steps = vad.shape[0] // train_batch\n",
        "test_steps = test.shape[0] // test_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifC3ViJVCynb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c8979ab8-dc21-4b67-88bf-2fe271811a4a"
      },
      "source": [
        "train.shape, vad.shape, test.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((70487, 112, 4), (9116, 112, 4), (8781, 112, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGcDkv-LJWUw",
        "colab_type": "text"
      },
      "source": [
        "###  Split data \n",
        "\n",
        "Each input sequence has both the features and labels (x and y, if you wish), so we have to separate them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk_8mzP9JrcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_features_labels(data, history, future):\n",
        "    \"\"\"\n",
        "    Method to separate historic and future events (features and labels). \n",
        "    It returns input data for the encoder and decoder, and the output data \n",
        "    for the decoder. The input data for the decoder is just the output data \n",
        "    of the decoder, shifted by one step. \n",
        "\n",
        "    :param data: numpy ndarray with sequences of history+future points, and 4 attributes \n",
        "      (time_interval, hour_of_day, day_of_week, sugar_level). It has shape (?, history+future, 4)\n",
        "    :param history: number of points for the features\n",
        "    :param future: number of points for the labels\n",
        "    :return three numpy arrays with the input data for the encoder (shape=(?, history, 4))\n",
        "        and decoder (shape=(?, future+1, 1)), and the output data for the decoder\n",
        "        (shape=(?, future+1, 1))\n",
        "    \"\"\"\n",
        "    # split features and labels . Note that for the later, we only keep the \n",
        "    # feature with the sugar level, which constitutes our target\n",
        "    yf, yl = data[:, :history], data[:, history:history+future, -1]\n",
        "    \n",
        "    # add start of sentence to labels (input to the decoder)\n",
        "    yl_input = np.zeros(shape=(yl.shape[0], yl.shape[1]+1))\n",
        "    yl_input[:, 1:] = yl\n",
        "    # add end of sentence to labels (output of the decoder)\n",
        "    yl_output = np.zeros(shape=(yl.shape[0], yl.shape[1]+1))\n",
        "    yl_output[:, :-1] = yl\n",
        "    \n",
        "    # add new dimension at the end of input/output arrays to the decoder\n",
        "    yl_input = yl_input[:, :, np.newaxis].astype(np.float32)\n",
        "    yl_output = yl_output[:, :, np.newaxis].astype(np.float32)\n",
        "    \n",
        "    return (yf, yl_input), yl_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE0-L8XQObFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = split_features_labels(train, history, future)\n",
        "vad_data = split_features_labels(vad, history, future)\n",
        "test_data = split_features_labels(test, history, future)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26Ib_uLdCyns",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "85817ec2-68e7-4927-9d89-d333b11abb70"
      },
      "source": [
        "train_data[0][0].shape, train_data[0][1].shape, train_data[1].shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((70487, 100, 4), (70487, 13, 1), (70487, 13, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGsfNjwfCyny",
        "colab_type": "text"
      },
      "source": [
        "### Make generators to feed the NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kSk9B0EPRkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_dataset(tensor):\n",
        "    return tf.data.Dataset.from_tensor_slices(tensor)\n",
        "\n",
        "def make_iterator(tensor, batch_size, num_epochs):\n",
        "    dataset = make_dataset(tensor)\n",
        "    return dataset.batch(batch_size).repeat(num_epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-QOHu8yPTh7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batch = 200\n",
        "vad_batch = 200\n",
        "test_batch = 200\n",
        "num_epochs = 5\n",
        "\n",
        "train_gen = make_iterator(train_data, train_batch, num_epochs)\n",
        "vad_gen = make_iterator(vad_data, vad_batch, num_epochs=1)\n",
        "test_gen = make_iterator(test_data, test_batch, num_epochs=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuVvMSenPYj8",
        "colab_type": "text"
      },
      "source": [
        "## Seq2seq model with RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeDOOy49Cyn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seq2seq(history, future, latent_dim):\n",
        "    clear_session()\n",
        "    # Define an input sequence and process it.\n",
        "    encoder_inputs = Input(shape=(history, 1))\n",
        "    encoder = LSTM(latent_dim, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "    # We discard `encoder_outputs` and only keep the states.\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Set up the decoder, using `encoder_states` as initial state.\n",
        "    decoder_inputs = Input(shape=(future+1, 1))\n",
        "    # We set up our decoder to return full output sequences,\n",
        "    # and to return internal states as well. We don't use the \n",
        "    # return states in the training model, but we will use them in inference.\n",
        "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                         initial_state=encoder_states)\n",
        "    decoder_dense = Dense(1, activation='selu')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Define the model that will turn\n",
        "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l1qvoAsCyn8",
        "colab_type": "code",
        "colab": {},
        "outputId": "c1088651-18b9-4806-8163-8a5b45782412"
      },
      "source": [
        "m2 = seq2seq(history, future, latent_dim=50)\n",
        "m2.compile(optimizer='rmsprop', loss=MAE, metrics=[MAPE, MAE])\n",
        "h2 = m2.fit(x=train_gen[0], y=train_gen[1], batch_size=train_batch, epochs=num_epochs, \n",
        "      validation_data=vad_gen)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 70350 samples, validate on 9365 samples\n",
            "Epoch 1/5\n",
            "70350/70350 [==============================] - 228s 3ms/step - loss: 0.1168 - mean_absolute_percentage_error: 28422752.0144 - mean_absolute_error: 0.1168 - val_loss: 0.0772 - val_mean_absolute_percentage_error: 18801599.7768 - val_mean_absolute_error: 0.0772\n",
            "Epoch 2/5\n",
            "70350/70350 [==============================] - 224s 3ms/step - loss: 0.0666 - mean_absolute_percentage_error: 17078473.5124 - mean_absolute_error: 0.0666 - val_loss: 0.0664 - val_mean_absolute_percentage_error: 10031653.0635 - val_mean_absolute_error: 0.0664\n",
            "Epoch 3/5\n",
            "70350/70350 [==============================] - 197s 3ms/step - loss: 0.0553 - mean_absolute_percentage_error: 13029406.1173 - mean_absolute_error: 0.0553 - val_loss: 0.0525 - val_mean_absolute_percentage_error: 14263808.8206 - val_mean_absolute_error: 0.0525\n",
            "Epoch 4/5\n",
            "70350/70350 [==============================] - 204s 3ms/step - loss: 0.0477 - mean_absolute_percentage_error: 10461799.7569 - mean_absolute_error: 0.0477 - val_loss: 0.0479 - val_mean_absolute_percentage_error: 15906579.9658 - val_mean_absolute_error: 0.0479\n",
            "Epoch 5/5\n",
            "70350/70350 [==============================] - 208s 3ms/step - loss: 0.0424 - mean_absolute_percentage_error: 7575081.4666 - mean_absolute_error: 0.0424 - val_loss: 0.0418 - val_mean_absolute_percentage_error: 4585248.3903 - val_mean_absolute_error: 0.0418\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt5Wfqm7CyoB",
        "colab_type": "code",
        "colab": {},
        "outputId": "cfcba8cd-1649-4ffc-c005-02dc24005cf3"
      },
      "source": [
        "m3 = seq2seq(history, future, latent_dim=50)\n",
        "m3.compile(optimizer='rmsprop', loss=MAPE, metrics=[MAPE, MAE])\n",
        "h3 = m3.fit(x=train_gen[0], y=train_gen[1], batch_size=train_batch, epochs=num_epochs, \n",
        "      validation_data=vad_gen)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 70350 samples, validate on 9365 samples\n",
            "Epoch 1/5\n",
            "70350/70350 [==============================] - 213s 3ms/step - loss: 734721.6606 - mean_absolute_percentage_error: 734721.6606 - mean_absolute_error: 0.4195 - val_loss: 169410.8792 - val_mean_absolute_percentage_error: 169410.8792 - val_mean_absolute_error: 0.4177\n",
            "Epoch 2/5\n",
            "70350/70350 [==============================] - 201s 3ms/step - loss: 401224.5604 - mean_absolute_percentage_error: 401224.5604 - mean_absolute_error: 0.4178 - val_loss: 617839.9356 - val_mean_absolute_percentage_error: 617839.9356 - val_mean_absolute_error: 0.4112\n",
            "Epoch 3/5\n",
            "70350/70350 [==============================] - 202s 3ms/step - loss: 309001.9542 - mean_absolute_percentage_error: 309001.9542 - mean_absolute_error: 0.4177 - val_loss: 105411.4461 - val_mean_absolute_percentage_error: 105411.4461 - val_mean_absolute_error: 0.4193\n",
            "Epoch 4/5\n",
            "70350/70350 [==============================] - 203s 3ms/step - loss: 249273.3744 - mean_absolute_percentage_error: 249273.3744 - mean_absolute_error: 0.4175 - val_loss: 327002.3357 - val_mean_absolute_percentage_error: 327002.3357 - val_mean_absolute_error: 0.4135\n",
            "Epoch 5/5\n",
            "70350/70350 [==============================] - 222s 3ms/step - loss: 214279.1935 - mean_absolute_percentage_error: 214279.1935 - mean_absolute_error: 0.4175 - val_loss: 245959.5692 - val_mean_absolute_percentage_error: 245959.5692 - val_mean_absolute_error: 0.4153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJ8rwomgCyoG",
        "colab_type": "code",
        "colab": {},
        "outputId": "6d46fc93-8e34-4f92-de4e-56a87dd0e4e3"
      },
      "source": [
        "m4 = seq2seq(history, future, latent_dim=100)\n",
        "m4.compile(optimizer='rmsprop', loss=MAE, metrics=[MAPE, MAE])\n",
        "h4 = m4.fit(x=train_gen[0], y=train_gen[1], batch_size=train_batch, epochs=num_epochs, \n",
        "      validation_data=vad_gen)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 70350 samples, validate on 9365 samples\n",
            "Epoch 1/5\n",
            "70350/70350 [==============================] - 407s 6ms/step - loss: 0.1085 - mean_absolute_percentage_error: 26867136.7138 - mean_absolute_error: 0.1085 - val_loss: 0.0746 - val_mean_absolute_percentage_error: 20523189.9221 - val_mean_absolute_error: 0.0746\n",
            "Epoch 2/5\n",
            "70350/70350 [==============================] - 401s 6ms/step - loss: 0.0642 - mean_absolute_percentage_error: 19557340.7932 - mean_absolute_error: 0.0642 - val_loss: 0.0657 - val_mean_absolute_percentage_error: 33651767.3134 - val_mean_absolute_error: 0.0657\n",
            "Epoch 3/5\n",
            "70350/70350 [==============================] - 401s 6ms/step - loss: 0.0505 - mean_absolute_percentage_error: 13386543.2793 - mean_absolute_error: 0.0505 - val_loss: 0.0428 - val_mean_absolute_percentage_error: 6656499.4861 - val_mean_absolute_error: 0.0428\n",
            "Epoch 4/5\n",
            "70350/70350 [==============================] - 398s 6ms/step - loss: 0.0425 - mean_absolute_percentage_error: 7110788.7436 - mean_absolute_error: 0.0425 - val_loss: 0.0388 - val_mean_absolute_percentage_error: 3930740.2112 - val_mean_absolute_error: 0.0388\n",
            "Epoch 5/5\n",
            "70350/70350 [==============================] - 367s 5ms/step - loss: 0.0385 - mean_absolute_percentage_error: 5725695.5741 - mean_absolute_error: 0.0385 - val_loss: 0.0423 - val_mean_absolute_percentage_error: 8874910.2440 - val_mean_absolute_error: 0.0423\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}